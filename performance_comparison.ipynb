{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "#from keras import backend as K\n",
    "#K.set_image_data_format('channels_last')\n",
    "from ImageDataAugmentor.image_data_augmentor import *\n",
    "import albumentations\n",
    "from datetime import datetime\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:                      Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\r\n"
     ]
    }
   ],
   "source": [
    "!lscpu | grep \"Model name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6812 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def image_preprocessing(x):\n",
    "    x = x/255. # rescale to [0,1]\n",
    "    return(x)\n",
    "    \n",
    "TEST_AUGMENTATIONS = albumentations.Compose([\n",
    "    albumentations.HorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "img_test_data_gen = ImageDataAugmentor(augment=TEST_AUGMENTATIONS, \n",
    "                                  augment_seed=123,\n",
    "                                  preprocess_input = image_preprocessing)\n",
    "img_test_gen = img_test_data_gen.flow_from_directory('./test_data/img', \n",
    "                                           class_mode=None, \n",
    "                                           shuffle=True, \n",
    "                                           seed=123, \n",
    "                                           color_mode='rgb', \n",
    "                                           target_size=(512, 512),\n",
    "                                           batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(model):\n",
    "    image_batch = next(img_test_gen)\n",
    "    for i in range(4):\n",
    "        start = datetime.now()\n",
    "        pred = model.predict_on_batch(np.expand_dims(image_batch[i,:,:,:], axis=0))\n",
    "        end = datetime.now()\n",
    "        print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performance on image 512*512\n",
    "\n",
    "## U-Net (resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.529350\n",
      "0:00:00.477358\n",
      "0:00:00.515232\n",
      "0:00:00.518919\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name='resnet18',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "    \n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net (resnet34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.173203\n",
      "0:00:00.558949\n",
      "0:00:00.599785\n",
      "0:00:00.569993\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name='resnet34',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "\n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net (resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.279154\n",
      "0:00:00.973155\n",
      "0:00:00.967939\n",
      "0:00:00.890561\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name='resnet50',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "\n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net (efn-B0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.942730\n",
      "0:00:00.778575\n",
      "0:00:00.721337\n",
      "0:00:00.747951\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name='efficientnetb0',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "\n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net (efn-B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.713695\n",
      "0:00:00.978065\n",
      "0:00:00.956792\n",
      "0:00:00.970785\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name='efficientnetb2',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "\n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net (mobilenet-v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gosha20777/anaconda3/envs/tf-1-14/lib/python3.7/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.919622\n",
      "0:00:00.656635\n",
      "0:00:00.602401\n",
      "0:00:00.575693\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name='mobilenet',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "\n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net (Custom-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Concatenate, Convolution2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback, ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def model1(img_rows, img_cols, optimizer, reg=0.1):\n",
    "    '''https://github.com/yihui-he/u-net'''\n",
    "    inputs = Input(shape=(img_rows, img_cols, 3))\n",
    "    bn1 = BatchNormalization()(inputs)\n",
    "    conv1 = Convolution2D(32, (3, 3), activation='relu', padding='same')(bn1)\n",
    "    conv1 = Convolution2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Convolution2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Convolution2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Convolution2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Convolution2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Convolution2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Convolution2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    conv5 = Convolution2D(512, (3, 3), activation='elu', padding='same')(pool4)\n",
    "    conv5 = Convolution2D(512, (3, 3), activation='elu', padding='same')(conv5)\n",
    "    up6 = Concatenate()([Convolution2D(256, (2, 2), activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv5)), conv4])\n",
    "    conv6 = Convolution2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Convolution2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    up7 = Concatenate()([Convolution2D(128, (2, 2),activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv6)), conv3])\n",
    "    conv7 = Convolution2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Convolution2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    up8 = Concatenate()([Convolution2D(64, (2, 2),activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv7)), conv2])\n",
    "    conv8 = Convolution2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Convolution2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    up9 = Concatenate()([Convolution2D(32, (2, 2),activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv8)), conv1])\n",
    "    conv9 = Convolution2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Convolution2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    bn = BatchNormalization()(conv9)    \n",
    "    conv10 = Convolution2D(1, (1, 1), activation='sigmoid')(bn)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    for layer in range(len(model.layers)):\n",
    "        model.layers[layer].kernel_regularizer=l2(reg)\n",
    "        model.layers[layer].bias_regularizer=l2(reg)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "\n",
    "    print('U-Net compiled,  batch_norm1=True, batch_norm10=True, 4 skips.')\n",
    "    print('Input shape:', model.input_shape)\n",
    "    print('Output shape:', model.output_shape)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net compiled,  batch_norm1=True, batch_norm10=True, 4 skips.\n",
      "Input shape: (None, 512, 512, 3)\n",
      "Output shape: (None, 512, 512, 1)\n",
      "0:00:03.602837\n",
      "0:00:00.623623\n",
      "0:00:00.623829\n",
      "0:00:00.640058\n"
     ]
    }
   ],
   "source": [
    "model = model1(512, 512, optimizer=Adam(learning_rate=1e-3))\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linknet (resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.386711\n",
      "0:00:00.106594\n",
      "0:00:00.107003\n",
      "0:00:00.112638\n"
     ]
    }
   ],
   "source": [
    "model = sm.Linknet(backbone_name='resnet18',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "\n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp5tw66lel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp5tw66lel/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS\n",
    "]\n",
    "tflite = converter.convert()\n",
    "\n",
    "with open(\"Linknet.tflite\",\"wb\") as h:\n",
    "    h.write(tflite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### int8 quanitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpqst0_2xg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpqst0_2xg/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calibration image test_data/img/1/146___p04982.jpg\n",
      "calibration image test_data/img/1/256___p04096.jpg\n",
      "calibration image test_data/img/1/77___n00110.jpg\n",
      "calibration image test_data/img/1/50___p00978.jpg\n",
      "calibration image test_data/img/1/32___n00341.jpg\n",
      "calibration image test_data/img/1/18___n02883.jpg\n",
      "calibration image test_data/img/1/329___p03688.jpg\n",
      "calibration image test_data/img/1/46___p05597.jpg\n",
      "calibration image test_data/img/1/296___p05680.jpg\n",
      "calibration image test_data/img/1/385___p04509.jpg\n"
     ]
    }
   ],
   "source": [
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "def representative_data_gen():\n",
    "    fimage = glob.glob('test_data/img/1/*.jpg')\n",
    "    for input_value in range(10):\n",
    "        if os.path.exists(fimage[input_value]):\n",
    "            original_image=cv2.imread(fimage[input_value])\n",
    "            image_data = cv2.resize(original_image, (512,512))\n",
    "            image_data = preprocess_image(image_data)\n",
    "            img_in = image_data[np.newaxis, ...].astype(np.float32)\n",
    "            print(\"calibration image {}\".format(fimage[input_value]))\n",
    "            yield [img_in]\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "#converter.target_spec.supported_ops = [\n",
    "#    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "#    tf.lite.OpsSet.SELECT_TF_OPS\n",
    "#]\n",
    "#converter.allow_custom_ops = True\n",
    "#converter.representative_dataset = representative_data_gen\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "\n",
    "tflite = converter.convert()\n",
    "\n",
    "with open(\"model_int8.tflite\",\"wb\") as h:\n",
    "    h.write(tflite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'data', 'index': 0, 'shape': array([  1, 512, 512,   3], dtype=int32), 'shape_signature': array([ -1, 512, 512,   3], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (1.006827473640442, -23), 'quantization_parameters': {'scales': array([1.0068275], dtype=float32), 'zero_points': array([-23], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "[{'name': 'Identity', 'index': 187, 'shape': array([1, 1, 1, 1], dtype=int32), 'shape_signature': array([-1, -1, -1,  1], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='model_int8.tflite')\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print(input_details)\n",
    "print(output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = cv2.imread('test_data/img/1/0___n04759.jpg')\n",
    "image_data = cv2.resize(original_image, (512,512))\n",
    "image_data = preprocess_image(image_data)\n",
    "#image_data = image_data / 255.\n",
    "image_data = np.expand_dims(image_data, axis=0)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:24.419191\n"
     ]
    }
   ],
   "source": [
    "def set_input_tensor(interpreter, input):\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    tensor_index = input_details['index']\n",
    "    input_tensor = interpreter.tensor(tensor_index)()[0]\n",
    "    # Inputs for the TFLite model must be uint8, so we quantize our input data.\n",
    "    # NOTE: This step is necessary only because we're receiving input data from\n",
    "    # ImageDataGenerator, which rescaled all image data to float [0,1]. When using\n",
    "    # bitmap inputs, they're already uint8 [0,255] so this can be replaced with:\n",
    "    #   input_tensor[:, :] = input\n",
    "    scale, zero_point = input_details['quantization']\n",
    "    input_tensor[:, :] = np.uint8(input / scale + zero_point)\n",
    "\n",
    "start = datetime.now()\n",
    "set_input_tensor(interpreter, image_data)\n",
    "#interpreter.set_tensor(input_details[0]['index'], image_data)\n",
    "interpreter.invoke()\n",
    "pred = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\n",
    "end = datetime.now()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras-Retinanet (resnet-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'Variable_5:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_6:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_7:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_8:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_9:0' shape=(9, 4) dtype=float32> anchors\n",
      "indices.shape[-1]: 1 <= params.rank: 2 and shape (?, 4)\n",
      "indices.shape[-1]: 1 <= params.rank: 1 and shape (?,)\n",
      "indices.shape[-1]: 2 <= params.rank: 2 and shape (?, 1)\n",
      "0:00:03.942450\n",
      "0:00:00.347900\n",
      "0:00:00.343302\n",
      "0:00:00.356467\n"
     ]
    }
   ],
   "source": [
    "from keras_retinanet import models\n",
    "\n",
    "def create_model(backbone_name, num_classes=1):\n",
    "    backbone_factory = models.backbone(backbone_name)\n",
    "    model = backbone_factory.retinanet(num_classes)\n",
    "    return models.convert_model(model)\n",
    "\n",
    "backbone = 'resnet50'\n",
    "model = create_model(backbone)\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'Variable_10:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_11:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_12:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_13:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_14:0' shape=(9, 4) dtype=float32> anchors\n",
      "indices.shape[-1]: 1 <= params.rank: 2 and shape (?, 4)\n",
      "indices.shape[-1]: 1 <= params.rank: 1 and shape (?,)\n",
      "indices.shape[-1]: 2 <= params.rank: 2 and shape (?, 1)\n",
      "0:00:07.703907\n",
      "0:00:00.541582\n",
      "0:00:00.542402\n",
      "0:00:00.551486\n"
     ]
    }
   ],
   "source": [
    "backbone = 'EfficientNetB0'\n",
    "model = create_model(backbone)\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras-Retinanet (mobilenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'Variable_10:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_11:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_12:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_13:0' shape=(9, 4) dtype=float32> anchors\n",
      "tracking <tf.Variable 'Variable_14:0' shape=(9, 4) dtype=float32> anchors\n",
      "indices.shape[-1]: 1 <= params.rank: 2 and shape (?, 4)\n",
      "indices.shape[-1]: 1 <= params.rank: 1 and shape (?,)\n",
      "indices.shape[-1]: 2 <= params.rank: 2 and shape (?, 1)\n",
      "0:00:03.493817\n",
      "0:00:00.174082\n",
      "0:00:00.245533\n",
      "0:00:00.190339\n"
     ]
    }
   ],
   "source": [
    "backbone = 'mobilenet224_0.1'\n",
    "model = create_model(backbone)\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO 4-tf (416*416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.658849\n",
      "0:00:00.255762\n",
      "0:00:00.254708\n",
      "0:00:00.265157\n"
     ]
    }
   ],
   "source": [
    "!cd ../tensorflow-yolov4-tflite && \\\n",
    "   python detect.py --weights ./checkpoints/yolov4-416 \\\n",
    "   --size 416 --model yolov4 \\\n",
    "   --image ../lacmus-research/test_data/img/1/0___n04759.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO 4-tf-tiny (416*416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.142459\n",
      "0:00:00.025333\n",
      "0:00:00.028778\n",
      "0:00:00.026272\n"
     ]
    }
   ],
   "source": [
    "!cd ../tensorflow-yolov4-tflite && \\\n",
    "   python detect.py --weights ./checkpoints/yolov4-tiny-416 \\\n",
    "   --size 416 --model yolov4 \\\n",
    "   --image ../lacmus-research/test_data/img/1/0___n04759.jpg --tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
