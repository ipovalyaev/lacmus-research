{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "#import segmentation_models as sm\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "from ImageDataAugmentor.image_data_augmentor import *\n",
    "import albumentations\n",
    "from datetime import datetime\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:                      Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz\r\n"
     ]
    }
   ],
   "source": [
    "!lscpu | grep \"Model name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6812 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def image_preprocessing(x):\n",
    "    x = x/255. # rescale to [0,1]\n",
    "    return(x)\n",
    "    \n",
    "TEST_AUGMENTATIONS = albumentations.Compose([\n",
    "    albumentations.HorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "img_test_data_gen = ImageDataAugmentor(augment=TEST_AUGMENTATIONS, \n",
    "                                  augment_seed=123,\n",
    "                                  preprocess_input = image_preprocessing)\n",
    "img_test_gen = img_test_data_gen.flow_from_directory('./test_data/img', \n",
    "                                           class_mode=None, \n",
    "                                           shuffle=True, \n",
    "                                           seed=123, \n",
    "                                           color_mode='rgb', \n",
    "                                           target_size=(512, 512),\n",
    "                                           batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(model):\n",
    "    image_batch = next(img_test_gen)\n",
    "    for i in range(4):\n",
    "        start = datetime.now()\n",
    "        pred = model.predict_on_batch(np.expand_dims(image_batch[i,:,:,:], axis=0))\n",
    "        end = datetime.now()\n",
    "        print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performance on image 512*512\n",
    "\n",
    "## U-Net (resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.529350\n",
      "0:00:00.477358\n",
      "0:00:00.515232\n",
      "0:00:00.518919\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name='resnet18',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "    \n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net (resnet34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02.173203\n",
      "0:00:00.558949\n",
      "0:00:00.599785\n",
      "0:00:00.569993\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name='resnet34',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "\n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net (resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.279154\n",
      "0:00:00.973155\n",
      "0:00:00.967939\n",
      "0:00:00.890561\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name='resnet50',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "\n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net (efn-B0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.942730\n",
      "0:00:00.778575\n",
      "0:00:00.721337\n",
      "0:00:00.747951\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name='efficientnetb0',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "\n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net (efn-B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.713695\n",
      "0:00:00.978065\n",
      "0:00:00.956792\n",
      "0:00:00.970785\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name='efficientnetb2',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "\n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net (mobilenet-v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gosha20777/anaconda3/envs/tf-1-14/lib/python3.7/site-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  warnings.warn('`input_shape` is undefined or non-square, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.919622\n",
      "0:00:00.656635\n",
      "0:00:00.602401\n",
      "0:00:00.575693\n"
     ]
    }
   ],
   "source": [
    "model = sm.Unet(backbone_name='mobilenet',\n",
    "    classes=1,\n",
    "    encoder_weights='imagenet',\n",
    "    input_shape=(512, 512, 3),\n",
    "    encoder_freeze=True)\n",
    "\n",
    "model.compile('Adam', \n",
    "              loss=sm.losses.binary_focal_dice_loss,\n",
    "              metrics=[sm.metrics.iou_score, sm.metrics.f1_score])\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net (Custom-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Concatenate, Convolution2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback, ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def model1(img_rows, img_cols, optimizer, reg=0.1):\n",
    "    '''https://github.com/yihui-he/u-net'''\n",
    "    inputs = Input(shape=(img_rows, img_cols, 3))\n",
    "    bn1 = BatchNormalization()(inputs)\n",
    "    conv1 = Convolution2D(32, (3, 3), activation='relu', padding='same')(bn1)\n",
    "    conv1 = Convolution2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Convolution2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Convolution2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Convolution2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Convolution2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Convolution2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Convolution2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    conv5 = Convolution2D(512, (3, 3), activation='elu', padding='same')(pool4)\n",
    "    conv5 = Convolution2D(512, (3, 3), activation='elu', padding='same')(conv5)\n",
    "    up6 = Concatenate()([Convolution2D(256, (2, 2), activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv5)), conv4])\n",
    "    conv6 = Convolution2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Convolution2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    up7 = Concatenate()([Convolution2D(128, (2, 2),activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv6)), conv3])\n",
    "    conv7 = Convolution2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Convolution2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    up8 = Concatenate()([Convolution2D(64, (2, 2),activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv7)), conv2])\n",
    "    conv8 = Convolution2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Convolution2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    up9 = Concatenate()([Convolution2D(32, (2, 2),activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv8)), conv1])\n",
    "    conv9 = Convolution2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Convolution2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    bn = BatchNormalization()(conv9)    \n",
    "    conv10 = Convolution2D(1, (1, 1), activation='sigmoid')(bn)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    for layer in range(len(model.layers)):\n",
    "        model.layers[layer].kernel_regularizer=l2(reg)\n",
    "        model.layers[layer].bias_regularizer=l2(reg)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "\n",
    "    print('U-Net compiled,  batch_norm1=True, batch_norm10=True, 4 skips.')\n",
    "    print('Input shape:', model.input_shape)\n",
    "    print('Output shape:', model.output_shape)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net compiled,  batch_norm1=True, batch_norm10=True, 4 skips.\n",
      "Input shape: (None, 512, 512, 3)\n",
      "Output shape: (None, 512, 512, 1)\n",
      "0:00:00.471553\n",
      "0:00:00.318344\n",
      "0:00:00.323204\n",
      "0:00:00.326768\n"
     ]
    }
   ],
   "source": [
    "model = model1(512, 512, optimizer=Adam(learning_rate=1e-3))\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras-Retinanet (resnet-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.348107\n",
      "0:00:00.281428\n",
      "0:00:00.280437\n",
      "0:00:00.280389\n"
     ]
    }
   ],
   "source": [
    "from keras_retinanet import models\n",
    "\n",
    "def create_model(backbone_name, num_classes=1):\n",
    "    backbone_factory = models.backbone(backbone_name)\n",
    "    model = backbone_factory.retinanet(num_classes)\n",
    "    return models.convert_model(model)\n",
    "\n",
    "backbone = 'resnet50'\n",
    "model = create_model(backbone)\n",
    "query(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#backbone = 'EfficientNetB0'\n",
    "#model = create_model(backbone)\n",
    "#query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras-Retinanet (mobilenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backbone = 'mobilenet224_0.1'\n",
    "#model = create_model(backbone)\n",
    "#query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO 4 (416*416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "nms iou: 0.413 score: 0.3\n",
      "(416, 416, 3)\n",
      "Found 6812 images belonging to 1 classes.\n",
      "0:00:01.488943\n",
      "0:00:00.285894\n",
      "0:00:00.287846\n",
      "0:00:00.278481\n"
     ]
    }
   ],
   "source": [
    "from yolo_4.models import Yolov4\n",
    "yolo = Yolov4(class_name_path = 'yolo_4/class_names/coco_classes.txt',\n",
    "               weight_path=None)\n",
    "\n",
    "print(yolo.img_size)\n",
    "model = yolo.inference_model\n",
    "\n",
    "def image_preprocessing(x):\n",
    "    x = x/255. # rescale to [0,1]\n",
    "    return(x)\n",
    "    \n",
    "TEST_AUGMENTATIONS = albumentations.Compose([\n",
    "    albumentations.HorizontalFlip(p=0.5)\n",
    "])\n",
    "\n",
    "yolo_img_test_data_gen = ImageDataAugmentor(augment=TEST_AUGMENTATIONS, \n",
    "                                  augment_seed=123,\n",
    "                                  preprocess_input = image_preprocessing)\n",
    "yolo_img_test_gen = yolo_img_test_data_gen.flow_from_directory('./test_data/img', \n",
    "                                           class_mode=None, \n",
    "                                           shuffle=True, \n",
    "                                           seed=123, \n",
    "                                           color_mode='rgb', \n",
    "                                           target_size=(416, 416),\n",
    "                                           batch_size=4)\n",
    "\n",
    "def yolo_query(model):\n",
    "    image_batch = next(yolo_img_test_gen)\n",
    "    for i in range(4):\n",
    "        start = datetime.now()\n",
    "        pred = model.predict_on_batch(np.expand_dims(image_batch[i,:,:,:], axis=0))\n",
    "        end = datetime.now()\n",
    "        print(end - start)\n",
    "\n",
    "yolo_query(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO 4-tf (416*416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.658849\n",
      "0:00:00.255762\n",
      "0:00:00.254708\n",
      "0:00:00.265157\n"
     ]
    }
   ],
   "source": [
    "!cd ../tensorflow-yolov4-tflite && \\\n",
    "   python detect.py --weights ./checkpoints/yolov4-416 \\\n",
    "   --size 416 --model yolov4 \\\n",
    "   --image ../lacmus-research/test_data/img/1/0___n04759.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO 4-tf-tiny (416*416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.142459\n",
      "0:00:00.025333\n",
      "0:00:00.028778\n",
      "0:00:00.026272\n"
     ]
    }
   ],
   "source": [
    "!cd ../tensorflow-yolov4-tflite && \\\n",
    "   python detect.py --weights ./checkpoints/yolov4-tiny-416 \\\n",
    "   --size 416 --model yolov4 \\\n",
    "   --image ../lacmus-research/test_data/img/1/0___n04759.jpg --tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
