{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6ffd37b-bfb4-4aa1-823f-a09f968793e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7e7460a-3e57-461f-85a8-4da1a67b24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ladd_path = '../../../ladd-and-weights/dataset/full_train_ds'\n",
    "ipsar_path = '../../../ladd-and-weights/dataset/3rd_party/heridal'\n",
    "ladd_anotation_path = os.path.join(ladd_path,'Annotations')\n",
    "ipsar_anotation_path = os.path.join(ipsar_path,'Annotations')\n",
    "target_path = '../../networks/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7cbb519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_LADD = True\n",
    "INCLUDE_IPSAR = True\n",
    "CROP_SIZE = 608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14496922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total annotations:  3203\n"
     ]
    }
   ],
   "source": [
    "annotation_files = {}\n",
    "if INCLUDE_LADD:\n",
    "    annotation_files.update({f.split('.xml')[0]:os.path.join (ladd_anotation_path,f) for f in os.listdir(ladd_anotation_path)})\n",
    "if INCLUDE_IPSAR:\n",
    "    annotation_files.update({f.split('.xml')[0]:os.path.join (ipsar_anotation_path,f) for f in os.listdir(ipsar_anotation_path)})\n",
    "print('total annotations: ',len(annotation_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b030236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images_path=os.path.join(target_path,'images')\n",
    "dataset_labels_path=os.path.join(target_path,'labels')\n",
    "os.makedirs(dataset_images_path,exist_ok=True)\n",
    "os.makedirs(dataset_labels_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "def036b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_around(img_size, xyxy):\n",
    "    left = max(0,xyxy[0]-random.randint(0,CROP_SIZE-xyxy[2]+xyxy[0]))\n",
    "    right = min(left+CROP_SIZE,img_size[0])\n",
    "    if right-left<CROP_SIZE and right==img_size[0]:\n",
    "        left=right-CROP_SIZE\n",
    "    top = max(0,xyxy[1] - random.randint(0,CROP_SIZE-xyxy[3]+xyxy[1]))\n",
    "    bottom = min(top+CROP_SIZE,img_size[1])\n",
    "    if bottom-top<CROP_SIZE and bottom==img_size[1]:\n",
    "        top=bottom-CROP_SIZE\n",
    "    return [left,top, right,bottom]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9496431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(file: str, target_set: str):\n",
    "    id=os.path.split(file)[-1]    \n",
    "    images_path_target=os.path.join(target_path,'images',target_set)\n",
    "    labels_path_target=os.path.join(target_path,'labels',target_set)\n",
    "    os.makedirs(images_path_target,exist_ok=True)\n",
    "    os.makedirs(labels_path_target, exist_ok=True)\n",
    "    points_xyxy=np.empty((0,4))\n",
    "    is_labels_found = False\n",
    "    if id in annotation_files and os.path.exists(annotation_files[id]):\n",
    "        img_size, points_xyxy=get_imgSize_and_list_of_yxyx(annotation_files[id])\n",
    "        points_xyxy=np.array(points_xyxy).reshape(-1,4)\n",
    "        is_labels_found = True\n",
    "    crop_idx = 0\n",
    "    for b in points_xyxy:\n",
    "        crop = get_crop_around(img_size,b)\n",
    "        cond_other_bb_in_crop = (crop[0]<= points_xyxy[:,::2].sum(axis=1)//2) &\\\n",
    "            (crop[2]>= points_xyxy[:,::2].sum(axis=1)//2) &\\\n",
    "            (crop[1]<= points_xyxy[:,1::2].sum(axis=1)//2) &\\\n",
    "            (crop[3]>= points_xyxy[:,1::2].sum(axis=1)//2) &\\\n",
    "            (b[0]!=points_xyxy[:,0])\n",
    "\n",
    "        target_pixel_boxes = [[\n",
    "            max(0,box[0]-crop[0]),\n",
    "            max(0,box[1]-crop[1]),\n",
    "            min(CROP_SIZE,box[2]-crop[0]),\n",
    "            min(CROP_SIZE,box[3]-crop[1])\n",
    "        ]for box in np.vstack([b,points_xyxy[cond_other_bb_in_crop,:]])]\n",
    "\n",
    "        target_boxes=[[\n",
    "            (box[0]+box[2])//2/CROP_SIZE,\n",
    "            (box[1]+box[3])//2/CROP_SIZE,\n",
    "            abs(box[0]-box[2])/CROP_SIZE,\n",
    "            abs(box[1]-box[3])/CROP_SIZE\n",
    "        ] for box in target_pixel_boxes]\n",
    "\n",
    "        target_lines = [\" \".join([\"0\",*map(str,b)])+'\\n' for b in target_boxes]\n",
    "\n",
    "        with open(os.path.join(labels_path_target,id+'_'+str(crop_idx)+'.txt'),'w') as f:\n",
    "            f.writelines(target_lines)\n",
    "        target_image_name = os.path.join(images_path_target,id+'_'+str(crop_idx)+'.jpg')\n",
    "        try:\n",
    "            cv2.imwrite(target_image_name, cv2.imread(file+'.jpg')[crop[1]:crop[3], crop[0]:crop[2]])\n",
    "        except Exception as e:\n",
    "            print(file,target_image_name,crop,cv2.imread(file+'.jpg').shape,e)\n",
    "\n",
    "        crop_idx+=1\n",
    "    if points_xyxy.shape[0]==0:\n",
    "        with open(os.path.join(labels_path_target,id+'_0'+'.txt'),'w') as f:\n",
    "            f.writelines(['\\n'])\n",
    "        target_image_name = os.path.join(images_path_target,id+'_0'+'.jpg')\n",
    "        im = cv2.imread(file+'.jpg')\n",
    "        x = random.randint(0,im.shape[1]-CROP_SIZE)\n",
    "        y = random.randint(0,im.shape[0]-CROP_SIZE)\n",
    "        cv2.imwrite(target_image_name,im[y:y+CROP_SIZE,x:x+CROP_SIZE])\n",
    "    return is_labels_found\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "330f4eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1311\n",
      "2894\n"
     ]
    }
   ],
   "source": [
    "trainset = []\n",
    "if INCLUDE_LADD:\n",
    "    with open(os.path.join(ladd_path,'ImageSets/Main/train.txt'),'r') as f:\n",
    "        trainset.extend([os.path.join(ladd_path,'JPEGImages',s.strip()) for s in f.readlines()])\n",
    "    print(len(trainset))\n",
    "if INCLUDE_IPSAR:\n",
    "    with open(os.path.join(ipsar_path,'ImageSets/Main/train.txt'),'r') as f:\n",
    "        trainset.extend([os.path.join(ipsar_path,'JPEGImages',s.strip()) for s in f.readlines()])\n",
    "    print(len(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "302cad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for file in trainset:\n",
    "    labels_missing = 0\n",
    "    if not split_image(file,'train'):\n",
    "        labels_missing+=1\n",
    "print(labels_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a34c3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = []\n",
    "if INCLUDE_LADD:\n",
    "    with open(os.path.join(ladd_path,'ImageSets/Main/val.txt'),'r') as f:\n",
    "        valset.extend([os.path.join(ladd_path,'JPEGImages',s.strip()) for s in f.readlines()])\n",
    "if INCLUDE_IPSAR:\n",
    "    with open(os.path.join(ipsar_path,'ImageSets/Main/val.txt'),'r') as f:\n",
    "        valset.extend([os.path.join(ipsar_path,'JPEGImages',s.strip()) for s in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ed543e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in valset:\n",
    "    split_image(file,'valid')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5bc5b443a3624a5fc3c89e6c5ef6facc856f2601421ec01dcd67b6b363ab748a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('lacmus')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
