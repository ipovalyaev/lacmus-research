{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4325e073-863a-497e-a56b-93d93a6d09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.detection.retinanet import RetinaNet\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
    "import  torchvision.transforms.functional as F\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "from torchvision.datasets.voc import VisionDataset\n",
    "\n",
    "from functions import *\n",
    "from functions_torch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3fc40b-344d-4d33-b1d8-be15eaeb2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['target_size']=(2000,1500)\n",
    "\n",
    "DSRoots = ['../../ladd-and-weights/dataset/'+d for d in\n",
    "    ['LADD/summer_nnovgorod_2021', 'LADD/spring_korolev_2019', 'LADD/summer_moscow_2019', 'LADD/summer_tambov_2019', 'LADD/winter_moscow_2018','full_train_ds' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2746332-fdec-4206-80a7-6cc9e8a59b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPU evaluation/test\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "                                                           min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "                                                           trainable_backbone_layers = 5)\n",
    "model.load_state_dict(torch.load('../../ladd-and-weights/weights/torch/experimental/resnet50_FRCNN_LADD_epoch_9.pth'), strict=True)\n",
    "# model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_FRCNN_LADD_epoch_9.pth'), strict=True)\n",
    "model = model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41841e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CPU evaluation/test\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "#                                                            min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "#                                                            trainable_backbone_layers = 5)\n",
    "# model.load_state_dict(torch.load('resnet50_FRCNN_LADD_epoch_9.pth', map_location=torch.device('cpu')), strict=True)\n",
    "# model = model.to(torch.device('cpu'))\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb1e6e2-ed83-4e42-82d0-e9a8bc76cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LADDDataSET(torchvision.datasets.VisionDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            image_set: str,\n",
    "            transforms: Optional[Callable] = None):     \n",
    "        super(LADDDataSET, self).__init__(root, transforms=transforms)\n",
    "        self.image_set = image_set\n",
    "\n",
    "        voc_root = root\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "        if not os.path.isdir(voc_root):\n",
    "            raise RuntimeError('Dataset not found or corrupted.')\n",
    "\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets/Main')\n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        assert (len(self.images) == len(self.annotations))\n",
    "        \n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a dictionary of the XML tree.\n",
    "        \"\"\"\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        description = LADDDataSET.parse_voc_xml(\n",
    "            ET.parse(self.annotations[index]).getroot())\n",
    "\n",
    "        # get bounding box coordinates \n",
    "        num_objs = len(description['annotation']['object'])\n",
    "        boxes = []\n",
    "        for l in description['annotation']['object']:\n",
    "            bb = l['bndbox']\n",
    "            boxes.append([int(bb['xmin']), int(bb['ymin']), int(bb['xmax']), int(bb['ymax'])])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)                \n",
    "        target[\"labels\"] = labels = torch.ones((num_objs,), dtype=torch.int64)  # there is only one class   \n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_voc_xml(node: ET.Element) -> Dict[str, Any]:\n",
    "        voc_dict: Dict[str, Any] = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
    "            for dc in map(LADDDataSET.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag == 'annotation':\n",
    "                def_dic['object'] = [def_dic['object']]\n",
    "            voc_dict = {\n",
    "                node.tag:\n",
    "                    {ind: v[0] if len(v) == 1 else v\n",
    "                     for ind, v in def_dic.items()}\n",
    "            }\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89c4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolo_to_pixels(size,yolo_string):\n",
    "    s=[float(s) for s in  yolo_string.strip().split() ]\n",
    "    center_x=int(s[1]*size[0])\n",
    "    center_y=int(s[2]*size[1])\n",
    "    w=int(s[3]*size[0])\n",
    "    h=int(s[4]*size[1])\n",
    "    x1=int(center_x-w/2)\n",
    "    x2=int(center_x+w/2)\n",
    "    y1=int(center_y-h/2)\n",
    "    y2=int(center_y+h/2)\n",
    "    return (x1,y1),(x2,y2),float(s[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51369137-7f61-45db-bf76-d1985adf8914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for ../../ladd-and-weights/dataset/LADD/summer_nnovgorod_2021, computing mAp : \n",
      "(0.26882707262684113, 0.17857142857142855)\n",
      "Yolo5\n",
      "(0.47312974920078166, 0.3125)\n",
      "Inference for ../../ladd-and-weights/dataset/LADD/spring_korolev_2019, computing mAp : \n",
      "(0.7382039705656762, 0.39102564102564097)\n",
      "Yolo5\n",
      "(0.643101950253254, 0.42194092827004226)\n",
      "Inference for ../../ladd-and-weights/dataset/LADD/summer_moscow_2019, computing mAp : \n",
      "(0.7921849366180698, 0.42948717948717946)\n",
      "Yolo5\n",
      "(0.8252919301107633, 0.6666666666666667)\n",
      "Inference for ../../ladd-and-weights/dataset/LADD/summer_tambov_2019, computing mAp : \n",
      "(0.857478241340623, 0.4875346260387812)\n",
      "Yolo5\n",
      "(0.9344204939972625, 0.7860262008733624)\n",
      "Inference for ../../ladd-and-weights/dataset/LADD/winter_moscow_2018, computing mAp : \n",
      "(0.9566707572592454, 0.5851528384279475)\n",
      "Yolo5\n",
      "(0.9486993499313932, 0.7478753541076487)\n",
      "Inference for ../../ladd-and-weights/dataset/full_train_ds, computing mAp : \n",
      "(0.820090417591393, 0.4446221511395441)\n",
      "Yolo5\n",
      "(0.864548570956989, 0.6509711595055915)\n"
     ]
    }
   ],
   "source": [
    "for ds in DSRoots:\n",
    "    dataset = LADDDataSET(ds,'test',get_transform(train=False,target_size=params['target_size'])) \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=1, shuffle=False, num_workers=1,collate_fn=collate_fn)\n",
    "    inference_res = evaluate(model,data_loader, device='cuda')\n",
    "    print(\"Inference for %s, computing mAp : \"%ds)\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    inference_res=[]\n",
    "\n",
    "    for index in range(len(dataset.annotations)):\n",
    "        labels_path= \"../labels\" #Путь к папке с предсказаниями \n",
    "        img_name = dataset.images[index].split('/')[-1].split('.')[0]\n",
    "        if (os.path.exists(os.path.join(labels_path,img_name+'.txt'))):\n",
    "            predictions_file = open(os.path.join(labels_path,img_name+'.txt'))\n",
    "            predictions = [str.strip(s) for s in predictions_file.readlines()]\n",
    "            predictions_file.close()\n",
    "        else:\n",
    "            predictions = []\n",
    "\n",
    "        description = LADDDataSET.parse_voc_xml(ET.parse(dataset.annotations[index]).getroot())\n",
    "\n",
    "        size = (int(description['annotation']['size']['width']),int(description['annotation']['size']['height']))\n",
    "        boxes = []\n",
    "        scores = []\n",
    "        outputs={}\n",
    "        for p in [convert_yolo_to_pixels(size,p) for p in predictions]:\n",
    "            boxes.append([p[0][0],p[0][1],p[1][0],p[1][1]])\n",
    "            scores.append(p[2])\n",
    "\n",
    "        outputs[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        outputs[\"scores\"] = torch.as_tensor(scores, dtype=torch.float32)\n",
    "        outputs[\"labels\"] = torch.ones((len(predictions),), dtype=torch.int64)\n",
    "\n",
    "        boxes = []\n",
    "\n",
    "        for l in description['annotation']['object']:\n",
    "            bb = l['bndbox']\n",
    "            boxes.append([int(bb['xmin']), int(bb['ymin']), int(bb['xmax']), int(bb['ymax'])])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.ones((len(description['annotation']['object']),), dtype=torch.int64)\n",
    "\n",
    "        res = [target], [outputs]\n",
    "        inference_res.append(res)\n",
    "    print('Yolo5')\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0d1bfd8-ec8c-4922-9b00-6a494f988211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python ./yolov5/detect.py --augment --weights ./yolov5/runs/train/exp3/weights/best.pt --source ../../../git/ladd-and-weights/dataset/full_train_ds/JPEGImages --imgsz 1984 --conf-thres 0.05 --iou-thres 0.01 --project predict --nosave --save-txt --save-conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29fbb82d-5c0c-4451-b167-873f4b45d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mv ./predict/exp/labels ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2c110-1a3c-4dbe-98f8-28dbb9d9e5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7e00b-334e-431d-84c2-ec669a31498f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
