{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325e073-863a-497e-a56b-93d93a6d09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.detection.retinanet import RetinaNet\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
    "import  torchvision.transforms.functional as F\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "from torchvision.datasets.voc import VisionDataset\n",
    "\n",
    "from functions import *\n",
    "from functions_torch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3fc40b-344d-4d33-b1d8-be15eaeb2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['target_size']=(2000,1500)\n",
    "\n",
    "DSRoots = ['../../ladd-and-weights/dataset/'+d for d in\n",
    "    ['LADD/summer_nnovgorod_2021', 'LADD/spring_korolev_2019', 'LADD/summer_moscow_2019', 'LADD/summer_tambov_2019', 'LADD/winter_moscow_2018','full_train_ds' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2746332-fdec-4206-80a7-6cc9e8a59b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPU evaluation/test\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "                                                           min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "                                                           trainable_backbone_layers = 5)\n",
    "model.load_state_dict(torch.load('../../ladd-and-weights/weights/torch/experimental/resnet50_FRCNN_LADD_epoch_9.pth'), strict=True)\n",
    "model = model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41841e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CPU evaluation/test\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "#                                                            min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "#                                                            trainable_backbone_layers = 5)\n",
    "# model.load_state_dict(torch.load('resnet50_FRCNN_LADD_epoch_9.pth', map_location=torch.device('cpu')), strict=True)\n",
    "# model = model.to(torch.device('cpu'))\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1e6e2-ed83-4e42-82d0-e9a8bc76cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LADDDataSET(torchvision.datasets.VisionDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            image_set: str,\n",
    "            transforms: Optional[Callable] = None):     \n",
    "        super(LADDDataSET, self).__init__(root, transforms=transforms)\n",
    "        self.image_set = image_set\n",
    "\n",
    "        voc_root = root\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "        if not os.path.isdir(voc_root):\n",
    "            raise RuntimeError('Dataset not found or corrupted.')\n",
    "\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets/Main')\n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        assert (len(self.images) == len(self.annotations))\n",
    "        \n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a dictionary of the XML tree.\n",
    "        \"\"\"\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        description = LADDDataSET.parse_voc_xml(\n",
    "            ET.parse(self.annotations[index]).getroot())\n",
    "\n",
    "        # get bounding box coordinates \n",
    "        num_objs = len(description['annotation']['object'])\n",
    "        boxes = []\n",
    "        for l in description['annotation']['object']:\n",
    "            bb = l['bndbox']\n",
    "            boxes.append([int(bb['xmin']), int(bb['ymin']), int(bb['xmax']), int(bb['ymax'])])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)                \n",
    "        target[\"labels\"] = labels = torch.ones((num_objs,), dtype=torch.int64)  # there is only one class   \n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_voc_xml(node: ET.Element) -> Dict[str, Any]:\n",
    "        voc_dict: Dict[str, Any] = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
    "            for dc in map(LADDDataSET.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag == 'annotation':\n",
    "                def_dic['object'] = [def_dic['object']]\n",
    "            voc_dict = {\n",
    "                node.tag:\n",
    "                    {ind: v[0] if len(v) == 1 else v\n",
    "                     for ind, v in def_dic.items()}\n",
    "            }\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolo_to_pixels(size,yolo_string):\n",
    "    s=[float(s) for s in  yolo_string.strip().split() ]\n",
    "    center_x=int(s[1]*size[0])\n",
    "    center_y=int(s[2]*size[1])\n",
    "    w=int(s[3]*size[0])\n",
    "    h=int(s[4]*size[1])\n",
    "    x1=int(center_x-w/2)\n",
    "    x2=int(center_x+w/2)\n",
    "    y1=int(center_y-h/2)\n",
    "    y2=int(center_y+h/2)\n",
    "    return (x1,y1),(x2,y2),float(s[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51369137-7f61-45db-bf76-d1985adf8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in DSRoots:\n",
    "    dataset = LADDDataSET(ds,'test',get_transform(train=False,target_size=params['target_size'])) \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=1, shuffle=False, num_workers=1,collate_fn=collate_fn)\n",
    "    inference_res = evaluate(model,data_loader, device='cpu')\n",
    "    print(\"Inference for %s, computing mAp : \"%ds)\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    inference_res=[]\n",
    "\n",
    "    for index in range(len(dataset.annotations)):\n",
    "        labels_path= \"../labels\" #Путь к папке с предсказаниями \n",
    "        img_name = dataset.images[index].split('/')[-1].split('.')[0]\n",
    "        if (os.path.exists(os.path.join(labels_path,img_name+'.txt'))):\n",
    "            predictions_file = open(os.path.join(labels_path,img_name+'.txt'))\n",
    "            predictions = [str.strip(s) for s in predictions_file.readlines()]\n",
    "            predictions_file.close()\n",
    "        else:\n",
    "            predictions = []\n",
    "\n",
    "        description = LADDDataSET.parse_voc_xml(ET.parse(dataset.annotations[index]).getroot())\n",
    "\n",
    "        size = (int(description['annotation']['size']['width']),int(description['annotation']['size']['height']))\n",
    "        boxes = []\n",
    "        scores = []\n",
    "        outputs={}\n",
    "        for p in [convert_yolo_to_pixels(size,p) for p in predictions]:\n",
    "            boxes.append([p[0][0],p[0][1],p[1][0],p[1][1]])\n",
    "            scores.append(p[2])\n",
    "\n",
    "        outputs[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        outputs[\"scores\"] = torch.as_tensor(scores, dtype=torch.float32)\n",
    "        outputs[\"labels\"] = torch.ones((len(predictions),), dtype=torch.int64)\n",
    "\n",
    "        boxes = []\n",
    "\n",
    "        for l in description['annotation']['object']:\n",
    "            bb = l['bndbox']\n",
    "            boxes.append([int(bb['xmin']), int(bb['ymin']), int(bb['xmax']), int(bb['ymax'])])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.ones((len(description['annotation']['object']),), dtype=torch.int64)\n",
    "\n",
    "        res = [target], [outputs]\n",
    "        inference_res.append(res)\n",
    "    print('Yolo5')\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
