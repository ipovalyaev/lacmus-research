{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4325e073-863a-497e-a56b-93d93a6d09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.detection.retinanet import RetinaNet\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
    "import  torchvision.transforms.functional as F\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "from torchvision.datasets.voc import VisionDataset\n",
    "\n",
    "from functions import *\n",
    "from functions_torch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3fc40b-344d-4d33-b1d8-be15eaeb2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['target_size']=(2000,1500)\n",
    "\n",
    "DSRoots = ['/app/host/lacmus/dataset/'+d for d in\n",
    "    ['full_lacmus_ds', 'spring_korolev_2019', 'summer_moscow_2019', 'summer_tambov_2019', 'winter_moscow_2018']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d56310-2094-489e-b0b4-9fe92b95ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=False, \n",
    "#                                                            min_size=params['target_size'][0], max_size = params['target_size'][1])\n",
    "# model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_LADD_epoch_8.pth'), strict=True)\n",
    "# model = model.to(torch.device('cuda'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2746332-fdec-4206-80a7-6cc9e8a59b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "                                                           min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "                                                           trainable_backbone_layers = 5)\n",
    "model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_FRCNN_LADD_epoch_9.pth'), strict=True)\n",
    "model = model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eb1e6e2-ed83-4e42-82d0-e9a8bc76cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LADDDataSET(torchvision.datasets.VisionDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            image_set: str,\n",
    "            transforms: Optional[Callable] = None):     \n",
    "        super(LADDDataSET, self).__init__(root, transforms=transforms)\n",
    "        self.image_set = image_set\n",
    "\n",
    "        voc_root = root\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "        if not os.path.isdir(voc_root):\n",
    "            raise RuntimeError('Dataset not found or corrupted.')\n",
    "\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets/Main')\n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        assert (len(self.images) == len(self.annotations))\n",
    "        \n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a dictionary of the XML tree.\n",
    "        \"\"\"\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        description = LADDDataSET.parse_voc_xml(\n",
    "            ET.parse(self.annotations[index]).getroot())\n",
    "\n",
    "        # get bounding box coordinates \n",
    "        num_objs = len(description['annotation']['object'])\n",
    "        boxes = []\n",
    "        for l in description['annotation']['object']:\n",
    "            bb = l['bndbox']\n",
    "            boxes.append([int(bb['xmin']), int(bb['ymin']), int(bb['xmax']), int(bb['ymax'])])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)                \n",
    "        target[\"labels\"] = labels = torch.ones((num_objs,), dtype=torch.int64)  # there is only one class   \n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_voc_xml(node: ET.Element) -> Dict[str, Any]:\n",
    "        voc_dict: Dict[str, Any] = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
    "            for dc in map(LADDDataSET.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag == 'annotation':\n",
    "                def_dic['object'] = [def_dic['object']]\n",
    "            voc_dict = {\n",
    "                node.tag:\n",
    "                    {ind: v[0] if len(v) == 1 else v\n",
    "                     for ind, v in def_dic.items()}\n",
    "            }\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51369137-7f61-45db-bf76-d1985adf8914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for /app/host/lacmus/dataset/full_lacmus_ds, computing mAp : \n",
      "0.9224656100089377\n",
      "0.8816261173472251\n",
      "Inference for /app/host/lacmus/dataset/spring_korolev_2019, computing mAp : \n",
      "0.7813762094212323\n",
      "0.6792287784603297\n",
      "Inference for /app/host/lacmus/dataset/summer_moscow_2019, computing mAp : \n",
      "0.8747487580439925\n",
      "0.8296626896177549\n",
      "Inference for /app/host/lacmus/dataset/summer_tambov_2019, computing mAp : \n",
      "0.9183631623027297\n",
      "0.8801822715839946\n",
      "Inference for /app/host/lacmus/dataset/winter_moscow_2018, computing mAp : \n",
      "0.9801216608440391\n",
      "0.9686321758051379\n"
     ]
    }
   ],
   "source": [
    "for ds in DSRoots:\n",
    "    dataset = LADDDataSET(ds,'test',get_transform(train=False,target_size=params['target_size'])) \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=1, shuffle=False, num_workers=1,collate_fn=collate_fn)\n",
    "    inference_res = evaluate(model,data_loader)\n",
    "    print(\"Inference for %s, computing mAp : \"%ds)\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.6, score_threshold = 0.05))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9644591-e50d-45e5-80b0-a9a5da2993f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
