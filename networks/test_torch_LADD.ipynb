{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f45ddd5-7de9-4bde-8a29-09a26d81c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.detection.retinanet import RetinaNet\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "import  torchvision.transforms.functional as F\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "from torchvision.datasets.voc import VisionDataset\n",
    "\n",
    "from functions import *\n",
    "from functions_torch import *\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd7e473-6220-4f7e-ac6b-c5736a3457e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['target_size']=(2000,1500)\n",
    "params['batch_size'] = 4\n",
    "params['lr'] = 0.001\n",
    "\n",
    "voc_root = '../../ladd-and-weights/dataset/full_train_ds'\n",
    "weights_root = '../../ladd-and-weights/weights/torch'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da6f2ee2-33f3-4296-9e6e-4af7f364466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reworked class from pytorch (see https://pytorch.org/vision/0.8/_modules/torchvision/datasets/voc.html#VOCDetection)\n",
    "\n",
    "class LADDDataSET(torchvision.datasets.VisionDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            image_set: str,\n",
    "            transforms: Callable,\n",
    "            transforms_wo_norm: Callable\n",
    "            ):     \n",
    "        super(LADDDataSET, self).__init__(root, transforms=transforms)\n",
    "        self.image_set = image_set\n",
    "\n",
    "        voc_root = root\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "        if not os.path.isdir(voc_root):\n",
    "            raise RuntimeError('Dataset not found or corrupted.')\n",
    "\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets/Main')\n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        self.transforms_wo_norm = transforms_wo_norm\n",
    "        assert (len(self.images) == len(self.annotations))\n",
    "        \n",
    "    def get_data(self, index: int):\n",
    "        # Read an image with OpenCV\n",
    "        image = cv2.imread(self.images[index])\n",
    "        # By default OpenCV uses BGR color space for color images,\n",
    "        # so we need to convert the image to RGB color space.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        description = LADDDataSET.parse_voc_xml(\n",
    "            ET.parse(self.annotations[index]).getroot())\n",
    "\n",
    "        # get bounding box coordinates \n",
    "        num_objs = len(description['annotation']['object'])\n",
    "        boxes = []\n",
    "        for l in description['annotation']['object']:\n",
    "            bb = l['bndbox']\n",
    "            boxes.append([int(bb['xmin']), int(bb['ymin']), int(bb['xmax']), int(bb['ymax']),1]) # 1 - for label\n",
    "        print (\"get_data boxes \", boxes, np.shape(image) )\n",
    "        return (image,boxes)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        (image,boxes,labels) = self.get_data(index)\n",
    "        augmented = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "        image = augmented['image']\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(augmented['bboxes'], dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(augmented['labels'],dtype=torch.int64)\n",
    "        return image, target\n",
    "    \n",
    "    def get_wo_norm(self, index: int):\n",
    "        \n",
    "        (image,boxes) = self.get_data(index)\n",
    "#         augmented = self.transforms_wo_norm(image=image, bboxes=boxes, labels=labels)\n",
    "        augmented = self.transforms_wo_norm(image=image, bboxes=boxes)\n",
    "\n",
    "        image = augmented['image']\n",
    "        print(\"albumented\")\n",
    "        print(augmented['bboxes'], augmented['image'].shape)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(augmented['bboxes'], dtype=torch.float32)          \n",
    "#         target[\"labels\"] = torch.as_tensor(augmented['labels'],dtype=torch.int64)\n",
    "        return image, target        \n",
    "    \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_voc_xml(node: ET.Element) -> Dict[str, Any]:\n",
    "        voc_dict: Dict[str, Any] = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
    "            for dc in map(LADDDataSET.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag == 'annotation':\n",
    "                def_dic['object'] = [def_dic['object']]\n",
    "            voc_dict = {\n",
    "                node.tag:\n",
    "                    {ind: v[0] if len(v) == 1 else v\n",
    "                     for ind, v in def_dic.items()}\n",
    "            }\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33e17d1c-0cdf-4c95-9622-fba7f3022ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pytorch implemenation of retinanet doesn't supports train on Images without any objects (which, probably need to be fixed)\n",
    "# # see https://github.com/pytorch/vision/blob/master/torchvision/models/detection/retinanet.py#L475\n",
    "# # As a temporary solution, yet, we just filtering out empty images\n",
    "\n",
    "# splits_dir = os.path.join(voc_root, 'ImageSets/Main') \n",
    "# annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "# with open(os.path.join(splits_dir,'train.txt'), \"r\") as f:\n",
    "#     file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "# non_empty = []\n",
    "# for a in file_names:\n",
    "#     description = LADDDataSET.parse_voc_xml(\n",
    "#         ET.parse(os.path.join(annotation_dir, a + \".xml\")).getroot()\n",
    "#     )\n",
    "#     num_objs = len(description['annotation']['object'])\n",
    "#     if num_objs > 0:\n",
    "#         non_empty.append(a+'\\n')\n",
    "        \n",
    "# with open(os.path.join(splits_dir,'train_non_empty.txt'), \"w\") as f:\n",
    "#     f.writelines(non_empty)\n",
    "\n",
    "# print('Total images '+str(len(file_names)), ' non empty: '+str(len(non_empty)))\n",
    "                                                \n",
    "                                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad793266-f8c9-4c47-bc8d-32c1b1628e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "albumentations_transform_train = A.Compose([\n",
    "    A.Resize(params['target_size'][0],params['target_size'][1]), \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ChannelShuffle(p=0.5),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',label_fields=['labels']))\n",
    "\n",
    "\n",
    "albumentations_transform_val = A.Compose([\n",
    "    A.Resize(params['target_size'][0],params['target_size'][1]), \n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',label_fields=['labels']))\n",
    "\n",
    "albumentations_transform_train_view = A.Compose([\n",
    "#     A.Resize(params['target_size'][0],params['target_size'][1]), \n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.ChannelShuffle(p=0.5),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "\n",
    "albumentations_transform_val_view = A.Compose([\n",
    "    A.Resize(params['target_size'][0],params['target_size'][1]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',label_fields=['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecd78b6f-b80d-4b32-a00f-7e558ff2c676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_data l =  {'name': 'Pedestrian', 'pose': 'Unspecified', 'truncated': '0', 'difficult': '0', 'bndbox': {'ymin': '175', 'xmin': '2001', 'ymax': '270', 'xmax': '2151'}}\n",
      "get_data l =  {'name': 'Pedestrian', 'pose': 'Unspecified', 'truncated': '0', 'difficult': '0', 'bndbox': {'ymin': '570', 'xmin': '1969', 'ymax': '669', 'xmax': '2089'}}\n",
      "get_data l =  {'name': 'Pedestrian', 'pose': 'Unspecified', 'truncated': '0', 'difficult': '0', 'bndbox': {'ymin': '1048', 'xmin': '2104', 'ymax': '1111', 'xmax': '2240'}}\n",
      "get_data l =  {'name': 'Pedestrian', 'pose': 'Unspecified', 'truncated': '0', 'difficult': '0', 'bndbox': {'ymin': '1624', 'xmin': '2062', 'ymax': '1663', 'xmax': '2155'}}\n",
      "get_data l =  {'name': 'Pedestrian', 'pose': 'Unspecified', 'truncated': '0', 'difficult': '0', 'bndbox': {'ymin': '2329', 'xmin': '2159', 'ymax': '2380', 'xmax': '2290'}}\n",
      "get_data boxes  [[2001, 175, 2151, 270, 1], [1969, 570, 2089, 669, 1], [2104, 1048, 2240, 1111, 1], [2062, 1624, 2155, 1663, 1], [2159, 2329, 2290, 2380, 1]] (3000, 4000, 3)\n",
      "albumented\n",
      "[(1500.75, 0.175, 1613.2499999999998, 0.27, 1), (1476.75, 0.5700000000000001, 1566.75, 0.669, 1), (1578.0, 1.048, 1680.0000000000002, 1.111, 1), (1546.4999999999998, 1.624, 1616.2499999999998, 1.663, 1), (1619.2499999999998, 2.3289999999999997, 1717.5, 2.38, 1)] torch.Size([3, 3000, 4000])\n"
     ]
    }
   ],
   "source": [
    "# # test DS\n",
    "im_idx = 99\n",
    "\n",
    "dataset = LADDDataSET(voc_root,'test',albumentations_transform_train,albumentations_transform_train_view) \n",
    "(image,target) = dataset.get_wo_norm(im_idx) \n",
    "im = F.to_pil_image(image)\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "for bb in target['boxes']:\n",
    "    draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "               (bb[2], bb[1]), (bb[0], bb[1])], width=4, fill=(255, 0, 0))\n",
    "\n",
    "# im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b74f2-3125-488e-8396-71806afa56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = LADDDataSET(voc_root,'train_non_empty',albumentations_transform_train) \n",
    "dataset_val = LADDDataSET(voc_root,'val',albumentations_transform_val) \n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=params['batch_size'], shuffle=True, num_workers=16\n",
    "     ,collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=16\n",
    "     ,collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2b766-ffa4-4902-b3ac-6ae5cbc46b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_sizes = tuple((x, int(x * 2 ** (1.0 / 3)), int(x * 2 ** (2.0 / 3))) for x in [16, 32, 64, 128, 256])\n",
    "aspect_ratios = ((0.5, 1.0, 2.0, 3.0),) * len(anchor_sizes)\n",
    "anchor_generator = AnchorGenerator(\n",
    "    anchor_sizes, aspect_ratios\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232f676-a422-4141-ae97-2ca0162a0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "                                                           min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "                                                           trainable_backbone_layers = 0, anchor_generator=anchor_generator)\n",
    "# Nees to define pretrained_backbone to use trainable_backbone_layers, otherwise it's ignored\n",
    "model.load_state_dict(torch.load(os.path.join(weights_root,'pretrain','resnet50_SDD.pth')), strict=False)\n",
    "# model.load_state_dict(torch.load(os.path.join(weights_root,'pretrain','resnet50_SDD.pth'),\n",
    "#             map_location=torch.device('cpu')\n",
    "#             ), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab91966-a90d-4336-8d91-8ac7f138631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "#                                                            min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "#                                                            trainable_backbone_layers = 0)\n",
    "# # Nees to define pretrained_backbone to use trainable_backbone_layers, otherwise it's ignored\n",
    "# model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_FRCNN_SDD_epoch_4.pth'), strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada00ab-91d8-4522-9a03-994bc3e4f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computation device\n",
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9, weight_decay=0.0005) \n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5f949-d04c-4427-9606-16794545f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10): # train without backbone\n",
    "\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n",
    "    print (\"Train done, evaluating.\")\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    inference_res = evaluate(model,data_loader_val)\n",
    "    print('Inference done, computing mAp : ')\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.6, score_threshold = 0.05))\n",
    "    print('Epoch Done')\n",
    "torch.save(model.state_dict(), 'resnet50_RetinaNet_LADD_head.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df264737-5016-48af-9036-b3ad68ca73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "del optimizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c30f30-5058-413c-ac00-5915a2ebfec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_sizes = tuple((x, int(x * 2 ** (1.0 / 3)), int(x * 2 ** (2.0 / 3))) for x in [16, 32, 64, 128, 256])\n",
    "aspect_ratios = ((0.5, 1.0, 2.0, 3.0),) * len(anchor_sizes)\n",
    "anchor_generator = AnchorGenerator(\n",
    "    anchor_sizes, aspect_ratios\n",
    ")\n",
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "                                                           min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "                                                           trainable_backbone_layers = 5, anchor_generator=anchor_generator)\n",
    "# model.load_state_dict(torch.load('resnet50_RetinaNet_LADD_head.pth'), strict=True)\n",
    "model.load_state_dict(torch.load('resnet50_FRCNN_LADD_epoch_9.pth'), strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89059b62-58cb-415a-acd9-2a5c15a4845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "#                                                            min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "#                                                            trainable_backbone_layers = 5)\n",
    "# model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_FRCNN_LADD_head.pth'), strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ba868-2f71-4220-a400-940a220eae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computation device\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9, weight_decay=0.0005) \n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=4,\n",
    "                                               gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1bd860-a88b-4f62-9fe3-4073cd5d013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10): # train with backbone now\n",
    "\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n",
    "    print (\"Train done, evaluating.\")\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    inference_res = evaluate(model,data_loader_val)\n",
    "    print('Inference done, computing mAp : ')\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.6, score_threshold = 0.05))\n",
    "    print('Epoch Done')\n",
    "    torch.save(model.state_dict(), 'resnet50_RN_LADD_epoch_%i.pth'%(epoch+10))\n",
    "\n",
    "# was    \n",
    "# Epoch: [9] Total time: 0:09:23 (0.4774 s / it)\n",
    "# Train done, evaluating.\n",
    "# Inference done, computing mAp : \n",
    "# 0.8940549518273289\n",
    "# 0.8334125499913912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e6d0f-88e9-4b3c-8b1e-f0095ea87e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to test evaluation model and show detections\n",
    "\n",
    "dataset_test = LADDDataSET(voc_root,'test',get_transform(train=False,target_size=params['target_size'])) \n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=1\n",
    "     ,collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "image_idx = 0\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "model.eval()\n",
    "for images, targets in data_loader_test:\n",
    "    g_images = list(img.to(device) for img in images)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    outputs = model(g_images)\n",
    "\n",
    "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "    res = targets, outputs\n",
    "    break\n",
    "\n",
    "\n",
    "im = F.to_pil_image(images[image_idx])\n",
    "targets\n",
    "# im = to_pil_image(dataset[10][0])\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "for idx in range(len(outputs[image_idx]['boxes'])):\n",
    "    width = math.ceil(outputs[image_idx]['scores'][idx]*10)\n",
    "    bb = outputs[0]['boxes'][idx]\n",
    "    draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "               (bb[2], bb[1]), (bb[0], bb[1])], width=width, fill=(255, 0, 0))\n",
    "\n",
    "for bb in targets[image_idx]['boxes'][:10]:\n",
    "    draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "               (bb[2], bb[1]), (bb[0], bb[1])], width=4, fill=(0,255, 0))\n",
    "im.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa697837-d2e5-4466-b110-0eb143b0978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img =  Image.open('..\\').convert('RGB')\n",
    "g_images = list(img.to(device) for img in images)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    outputs = model(g_images)\n",
    "\n",
    "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "    res = targets, outputs\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
