{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f45ddd5-7de9-4bde-8a29-09a26d81c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.detection.retinanet import RetinaNet\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "import  torchvision.transforms.functional as F\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "from torchvision.datasets.voc import VisionDataset\n",
    "\n",
    "from functions import *\n",
    "from functions_torch import *\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd7e473-6220-4f7e-ac6b-c5736a3457e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['target_size']=(2000,1500)\n",
    "params['batch_size'] = 4\n",
    "params['lr'] = 0.001\n",
    "\n",
    "voc_root = '../../ladd-and-weights/dataset/full_train_ds'\n",
    "weights_root = '../../ladd-and-weights/weights/torch'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6f2ee2-33f3-4296-9e6e-4af7f364466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reworked class from pytorch (see https://pytorch.org/vision/0.8/_modules/torchvision/datasets/voc.html#VOCDetection)\n",
    "\n",
    "class LADDDataSET(torchvision.datasets.VisionDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            image_set: str,\n",
    "            transforms: Callable,\n",
    "            transforms_wo_norm: Callable\n",
    "            ):     \n",
    "        super(LADDDataSET, self).__init__(root, transforms=transforms)\n",
    "        self.image_set = image_set\n",
    "\n",
    "        voc_root = root\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "        if not os.path.isdir(voc_root):\n",
    "            raise RuntimeError('Dataset not found or corrupted.')\n",
    "\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets/Main')\n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        self.transforms_wo_norm = transforms_wo_norm\n",
    "        assert (len(self.images) == len(self.annotations))\n",
    "        \n",
    "    def get_data(self, index: int, transforms: Callable):\n",
    "        # Read an image with OpenCV\n",
    "        image = cv2.imread(self.images[index])\n",
    "        # By default OpenCV uses BGR color space for color images,\n",
    "        # so we need to convert the image to RGB color space.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        description = LADDDataSET.parse_voc_xml(\n",
    "            ET.parse(self.annotations[index]).getroot())\n",
    "\n",
    "        # get bounding box coordinates \n",
    "        num_objs = len(description['annotation']['object'])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for l in description['annotation']['object']:\n",
    "            bb = l['bndbox']\n",
    "            boxes.append([int(bb['xmin']), int(bb['ymin']), int(bb['xmax']), int(bb['ymax'])]) \n",
    "            labels.append(1)\n",
    "        augmented = transforms(image=image, bboxes=boxes, labels=labels)\n",
    "        image = augmented['image']\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(augmented['bboxes'], dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(augmented['labels'],dtype=torch.int64)\n",
    "        \n",
    "        return image,target\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.get_data(index,self.transforms)\n",
    "        \n",
    "    \n",
    "    def get_wo_norm(self, index: int):        \n",
    "        return self.get_data(index, self.transforms_wo_norm)\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_voc_xml(node: ET.Element) -> Dict[str, Any]:\n",
    "        voc_dict: Dict[str, Any] = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
    "            for dc in map(LADDDataSET.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag == 'annotation':\n",
    "                def_dic['object'] = [def_dic['object']]\n",
    "            voc_dict = {\n",
    "                node.tag:\n",
    "                    {ind: v[0] if len(v) == 1 else v\n",
    "                     for ind, v in def_dic.items()}\n",
    "            }\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e17d1c-0cdf-4c95-9622-fba7f3022ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pytorch implemenation of retinanet doesn't supports train on Images without any objects (which, probably need to be fixed)\n",
    "# # see https://github.com/pytorch/vision/blob/master/torchvision/models/detection/retinanet.py#L475\n",
    "# # As a temporary solution, yet, we just filtering out empty images\n",
    "\n",
    "# splits_dir = os.path.join(voc_root, 'ImageSets/Main') \n",
    "# annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "# with open(os.path.join(splits_dir,'train.txt'), \"r\") as f:\n",
    "#     file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "# non_empty = []\n",
    "# for a in file_names:\n",
    "#     description = LADDDataSET.parse_voc_xml(\n",
    "#         ET.parse(os.path.join(annotation_dir, a + \".xml\")).getroot()\n",
    "#     )\n",
    "#     num_objs = len(description['annotation']['object'])\n",
    "#     if num_objs > 0:\n",
    "#         non_empty.append(a+'\\n')\n",
    "        \n",
    "# with open(os.path.join(splits_dir,'train_non_empty.txt'), \"w\") as f:\n",
    "#     f.writelines(non_empty)\n",
    "\n",
    "# print('Total images '+str(len(file_names)), ' non empty: '+str(len(non_empty)))\n",
    "                                                \n",
    "                                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad793266-f8c9-4c47-bc8d-32c1b1628e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "albumentations_transform_train = A.Compose([\n",
    "    A.Resize(params['target_size'][0],params['target_size'][1]), \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ChannelShuffle(p=0.5),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',label_fields=['labels']))\n",
    "\n",
    "\n",
    "albumentations_transform_val = A.Compose([\n",
    "    A.Resize(params['target_size'][0],params['target_size'][1]), \n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',label_fields=['labels']))\n",
    "\n",
    "albumentations_transform_train_view = A.Compose([\n",
    "    A.Resize(params['target_size'][0],params['target_size'][1]), \n",
    "    A.HorizontalFlip(p=1.0),\n",
    "    A.ChannelShuffle(p=1.0),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',label_fields=['labels']))\n",
    "\n",
    "albumentations_transform_val_view = A.Compose([\n",
    "    A.Resize(params['target_size'][0],params['target_size'][1]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',label_fields=['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd78b6f-b80d-4b32-a00f-7e558ff2c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test DS\n",
    "im_idx = 99\n",
    "\n",
    "dataset = LADDDataSET(voc_root,'test',albumentations_transform_train,albumentations_transform_train_view) \n",
    "(image,target) = dataset.get_wo_norm(im_idx) \n",
    "im = F.to_pil_image(image)\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "for bb in target['boxes']:\n",
    "    draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "               (bb[2], bb[1]), (bb[0], bb[1])], width=4, fill=(255, 0, 0))\n",
    "\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502b74f2-3125-488e-8396-71806afa56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = LADDDataSET(voc_root,'train_non_empty',albumentations_transform_train,\n",
    "                           albumentations_transform_train_view) \n",
    "dataset_val = LADDDataSET(voc_root,'val',albumentations_transform_val, \n",
    "                          albumentations_transform_val_view) \n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=params['batch_size'], shuffle=True, num_workers=4\n",
    "     ,collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=16\n",
    "     ,collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a2b766-ffa4-4902-b3ac-6ae5cbc46b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_sizes = tuple((x, int(x * 2 ** (1.0 / 3)), int(x * 2 ** (2.0 / 3))) for x in [16, 32, 64, 128, 256])\n",
    "aspect_ratios = ((0.5, 1.0, 2.0, 3.0),) * len(anchor_sizes)\n",
    "anchor_generator = AnchorGenerator(\n",
    "    anchor_sizes, aspect_ratios\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8232f676-a422-4141-ae97-2ca0162a0426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "                                                           min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "                                                           trainable_backbone_layers = 0, anchor_generator=anchor_generator)\n",
    "# Nees to define pretrained_backbone to use trainable_backbone_layers, otherwise it's ignored\n",
    "model.load_state_dict(torch.load(os.path.join(weights_root,'pretrain','resnet50_SDD.pth')), strict=False)\n",
    "# model.load_state_dict(torch.load(os.path.join(weights_root,'pretrain','resnet50_SDD.pth'),\n",
    "#             map_location=torch.device('cpu')\n",
    "#             ), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ab91966-a90d-4336-8d91-8ac7f138631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "#                                                            min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "#                                                            trainable_backbone_layers = 0)\n",
    "# # Nees to define pretrained_backbone to use trainable_backbone_layers, otherwise it's ignored\n",
    "# model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_FRCNN_SDD_epoch_4.pth'), strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ada00ab-91d8-4522-9a03-994bc3e4f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computation device\n",
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9, weight_decay=0.0005) \n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5f949-d04c-4427-9606-16794545f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/295]  eta: 0:11:47  lr: 0.000004  loss: 2.2853 (2.2853)  classification: 1.5119 (1.5119)  bbox_regression: 0.7735 (0.7735)  time: 2.3986  data: 1.2279  max mem: 3469\n",
      "Epoch: [0]  [100/295]  eta: 0:03:47  lr: 0.000344  loss: 1.4946 (1.8446)  classification: 0.8052 (1.1239)  bbox_regression: 0.6467 (0.7208)  time: 1.1675  data: 0.0234  max mem: 3547\n",
      "Epoch: [0]  [200/295]  eta: 0:01:50  lr: 0.000684  loss: 1.2573 (1.6019)  classification: 0.6739 (0.9456)  bbox_regression: 0.5788 (0.6563)  time: 1.1734  data: 0.0235  max mem: 3577\n",
      "Epoch: [0]  [294/295]  eta: 0:00:01  lr: 0.001000  loss: 1.2312 (1.4785)  classification: 0.6612 (0.8508)  bbox_regression: 0.5538 (0.6277)  time: 1.1752  data: 0.0240  max mem: 8164\n",
      "Epoch: [0] Total time: 0:05:45 (1.1712 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.21985391095081178, 0.026562345902232158)\n",
      "(0.0741508144717369, 0.016831585522206514)\n",
      "Epoch Done\n",
      "Epoch: [1]  [  0/295]  eta: 0:13:21  lr: 0.001000  loss: 1.6292 (1.6292)  classification: 1.0515 (1.0515)  bbox_regression: 0.5777 (0.5777)  time: 2.7159  data: 1.5292  max mem: 8164\n",
      "Epoch: [1]  [100/295]  eta: 0:03:49  lr: 0.001000  loss: 0.9972 (1.1200)  classification: 0.4894 (0.5797)  bbox_regression: 0.5217 (0.5403)  time: 1.1639  data: 0.0225  max mem: 8164\n",
      "Epoch: [1]  [200/295]  eta: 0:01:51  lr: 0.001000  loss: 1.0484 (1.1061)  classification: 0.4901 (0.5641)  bbox_regression: 0.5395 (0.5420)  time: 1.1634  data: 0.0223  max mem: 8164\n",
      "Epoch: [1]  [294/295]  eta: 0:00:01  lr: 0.001000  loss: 0.9748 (1.0953)  classification: 0.4789 (0.5578)  bbox_regression: 0.4944 (0.5374)  time: 1.1697  data: 0.0228  max mem: 8164\n",
      "Epoch: [1] Total time: 0:05:44 (1.1694 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.37931335787895204, 0.032012363395518266)\n",
      "(0.18785144872568227, 0.02178312543695036)\n",
      "Epoch Done\n",
      "Epoch: [2]  [  0/295]  eta: 0:12:23  lr: 0.001000  loss: 1.1972 (1.1972)  classification: 0.5851 (0.5851)  bbox_regression: 0.6121 (0.6121)  time: 2.5203  data: 1.3282  max mem: 8164\n",
      "Epoch: [2]  [100/295]  eta: 0:03:49  lr: 0.001000  loss: 1.0082 (1.0471)  classification: 0.5273 (0.5384)  bbox_regression: 0.4737 (0.5087)  time: 1.1621  data: 0.0227  max mem: 8164\n",
      "Epoch: [2]  [200/295]  eta: 0:01:51  lr: 0.001000  loss: 0.9762 (1.0042)  classification: 0.4894 (0.5050)  bbox_regression: 0.4948 (0.4992)  time: 1.1654  data: 0.0232  max mem: 8164\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10): # train without backbone\n",
    "\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n",
    "    print (\"Train done, evaluating.\")\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    inference_res = evaluate(model,data_loader_val)\n",
    "    print('Inference done, computing mAp : ')\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.6, score_threshold = 0.05))\n",
    "    print('Epoch Done')\n",
    "torch.save(model.state_dict(), 'resnet50_RetinaNet_LADD_head.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df264737-5016-48af-9036-b3ad68ca73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "del optimizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c30f30-5058-413c-ac00-5915a2ebfec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_sizes = tuple((x, int(x * 2 ** (1.0 / 3)), int(x * 2 ** (2.0 / 3))) for x in [16, 32, 64, 128, 256])\n",
    "aspect_ratios = ((0.5, 1.0, 2.0, 3.0),) * len(anchor_sizes)\n",
    "anchor_generator = AnchorGenerator(\n",
    "    anchor_sizes, aspect_ratios\n",
    ")\n",
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "                                                           min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "                                                           trainable_backbone_layers = 5, anchor_generator=anchor_generator)\n",
    "# model.load_state_dict(torch.load('resnet50_RetinaNet_LADD_head.pth'), strict=True)\n",
    "model.load_state_dict(torch.load('resnet50_FRCNN_LADD_epoch_9.pth'), strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89059b62-58cb-415a-acd9-2a5c15a4845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "#                                                            min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "#                                                            trainable_backbone_layers = 5)\n",
    "# model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_FRCNN_LADD_head.pth'), strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ba868-2f71-4220-a400-940a220eae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computation device\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9, weight_decay=0.0005) \n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=4,\n",
    "                                               gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1bd860-a88b-4f62-9fe3-4073cd5d013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10): # train with backbone now\n",
    "\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n",
    "    print (\"Train done, evaluating.\")\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    inference_res = evaluate(model,data_loader_val)\n",
    "    print('Inference done, computing mAp : ')\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.6, score_threshold = 0.05))\n",
    "    print('Epoch Done')\n",
    "    torch.save(model.state_dict(), 'resnet50_RN_LADD_epoch_%i.pth'%(epoch+10))\n",
    "\n",
    "# was    \n",
    "# Epoch: [9] Total time: 0:09:23 (0.4774 s / it)\n",
    "# Train done, evaluating.\n",
    "# Inference done, computing mAp : \n",
    "# 0.8940549518273289\n",
    "# 0.8334125499913912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e6d0f-88e9-4b3c-8b1e-f0095ea87e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to test evaluation model and show detections\n",
    "\n",
    "dataset_test = LADDDataSET(voc_root,'test',get_transform(train=False,target_size=params['target_size'])) \n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=1\n",
    "     ,collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "image_idx = 0\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "model.eval()\n",
    "for images, targets in data_loader_test:\n",
    "    g_images = list(img.to(device) for img in images)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    outputs = model(g_images)\n",
    "\n",
    "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "    res = targets, outputs\n",
    "    break\n",
    "\n",
    "\n",
    "im = F.to_pil_image(images[image_idx])\n",
    "targets\n",
    "# im = to_pil_image(dataset[10][0])\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "for idx in range(len(outputs[image_idx]['boxes'])):\n",
    "    width = math.ceil(outputs[image_idx]['scores'][idx]*10)\n",
    "    bb = outputs[0]['boxes'][idx]\n",
    "    draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "               (bb[2], bb[1]), (bb[0], bb[1])], width=width, fill=(255, 0, 0))\n",
    "\n",
    "for bb in targets[image_idx]['boxes'][:10]:\n",
    "    draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "               (bb[2], bb[1]), (bb[0], bb[1])], width=4, fill=(0,255, 0))\n",
    "im.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa697837-d2e5-4466-b110-0eb143b0978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img =  Image.open('..\\').convert('RGB')\n",
    "g_images = list(img.to(device) for img in images)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    outputs = model(g_images)\n",
    "\n",
    "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "    res = targets, outputs\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
