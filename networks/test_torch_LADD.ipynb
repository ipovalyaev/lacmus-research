{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f45ddd5-7de9-4bde-8a29-09a26d81c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.detection.retinanet import RetinaNet\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
    "\n",
    "import  torchvision.transforms.functional as F\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "from torchvision.datasets.voc import VisionDataset\n",
    "\n",
    "from functions import *\n",
    "from functions_torch import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7e473-6220-4f7e-ac6b-c5736a3457e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['target_size']=(2000,1500)\n",
    "params['batch_size'] = 8\n",
    "params['lr'] = 0.001\n",
    "\n",
    "voc_root = '/app/host/lacmus/git/ladd-and-weights/dataset/full_train_ds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6f2ee2-33f3-4296-9e6e-4af7f364466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reworked class from pytorch (see https://pytorch.org/vision/0.8/_modules/torchvision/datasets/voc.html#VOCDetection)\n",
    "\n",
    "class LADDDataSET(torchvision.datasets.VisionDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            image_set: str,\n",
    "            transforms: Optional[Callable] = None):     \n",
    "        super(LADDDataSET, self).__init__(root, transforms=transforms)\n",
    "        self.image_set = image_set\n",
    "\n",
    "        voc_root = root\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "        if not os.path.isdir(voc_root):\n",
    "            raise RuntimeError('Dataset not found or corrupted.')\n",
    "\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets/Main')\n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        assert (len(self.images) == len(self.annotations))\n",
    "        \n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a dictionary of the XML tree.\n",
    "        \"\"\"\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        description = LADDDataSET.parse_voc_xml(\n",
    "            ET.parse(self.annotations[index]).getroot())\n",
    "\n",
    "        # get bounding box coordinates \n",
    "        num_objs = len(description['annotation']['object'])\n",
    "        boxes = []\n",
    "        for l in description['annotation']['object']:\n",
    "            bb = l['bndbox']\n",
    "            boxes.append([int(bb['xmin']), int(bb['ymin']), int(bb['xmax']), int(bb['ymax'])])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)         # there is only one class            \n",
    "        target[\"labels\"] = labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_voc_xml(node: ET.Element) -> Dict[str, Any]:\n",
    "        voc_dict: Dict[str, Any] = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
    "            for dc in map(LADDDataSET.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag == 'annotation':\n",
    "                def_dic['object'] = [def_dic['object']]\n",
    "            voc_dict = {\n",
    "                node.tag:\n",
    "                    {ind: v[0] if len(v) == 1 else v\n",
    "                     for ind, v in def_dic.items()}\n",
    "            }\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e17d1c-0cdf-4c95-9622-fba7f3022ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch implemenation of retinanet doesn't supports train on Images without any objects (which, probably need to be fixed)\n",
    "# see https://github.com/pytorch/vision/blob/master/torchvision/models/detection/retinanet.py#L475\n",
    "# As a temporary solution, yet, we just filtering out empty images\n",
    "\n",
    "splits_dir = os.path.join(voc_root, 'ImageSets/Main') \n",
    "annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "with open(os.path.join(splits_dir,'train.txt'), \"r\") as f:\n",
    "    file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "non_empty = []\n",
    "for a in file_names:\n",
    "    description = LADDDataSET.parse_voc_xml(\n",
    "        ET.parse(os.path.join(annotation_dir, a + \".xml\")).getroot()\n",
    "    )\n",
    "    num_objs = len(description['annotation']['object'])\n",
    "    if num_objs > 0:\n",
    "        non_empty.append(a+'\\n')\n",
    "        \n",
    "with open(os.path.join(splits_dir,'train_non_empty.txt'), \"w\") as f:\n",
    "    f.writelines(non_empty)\n",
    "\n",
    "print('Total images '+str(len(file_names)), ' non empty: '+str(len(non_empty)))\n",
    "                                                \n",
    "                                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd78b6f-b80d-4b32-a00f-7e558ff2c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test DS\n",
    "# im_idx = 99\n",
    "\n",
    "# dataset = LADDDataSET('/app/host/lacmus/dataset/full_lacmus_ds','test',get_transform(train=True,target_size=params['target_size'])) \n",
    "# (image,target) = dataset[im_idx] \n",
    "# im = F.to_pil_image(image)\n",
    "# draw = ImageDraw.Draw(im)\n",
    "\n",
    "# for bb in target['boxes']:\n",
    "#     draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "#                (bb[2], bb[1]), (bb[0], bb[1])], width=4, fill=(255, 0, 0))\n",
    "\n",
    "# im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b74f2-3125-488e-8396-71806afa56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = LADDDataSET(voc_root,'train_non_empty',get_transform(train=True,target_size=params['target_size'])) \n",
    "dataset_val = LADDDataSET(voc_root,'val',get_transform(train=False,target_size=params['target_size'])) \n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=params['batch_size'], shuffle=True, num_workers=4\n",
    "     ,collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=4\n",
    "     ,collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8232f676-a422-4141-ae97-2ca0162a0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "#                                                            min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "#                                                            trainable_backbone_layers = 0)\n",
    "# # Nees to define pretrained_backbone to use trainable_backbone_layers, otherwise it's ignored\n",
    "# model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_SDD_epoch_8.pth'), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab91966-a90d-4336-8d91-8ac7f138631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "                                                           min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "                                                           trainable_backbone_layers = 0)\n",
    "# Nees to define pretrained_backbone to use trainable_backbone_layers, otherwise it's ignored\n",
    "model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_FRCNN_SDD_epoch_4.pth'), strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada00ab-91d8-4522-9a03-994bc3e4f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computation device\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9, weight_decay=0.0005) \n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5f949-d04c-4427-9606-16794545f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10): # train without backbone\n",
    "\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n",
    "    print (\"Train done, evaluating.\")\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    inference_res = evaluate(model,data_loader_val)\n",
    "    print('Inference done, computing mAp : ')\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.6, score_threshold = 0.05))\n",
    "    print('Epoch Done')\n",
    "torch.save(model.state_dict(), '/app/host/lacmus/weights/resnet50_FRCNN_LADD_head.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df264737-5016-48af-9036-b3ad68ca73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "del optimizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c30f30-5058-413c-ac00-5915a2ebfec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "#                                                            min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "#                                                            trainable_backbone_layers = 5)\n",
    "# model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_LADD_head.pth'), strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89059b62-58cb-415a-acd9-2a5c15a4845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "                                                           min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "                                                           trainable_backbone_layers = 5)\n",
    "model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_FRCNN_LADD_head.pth'), strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ba868-2f71-4220-a400-940a220eae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computation device\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9, weight_decay=0.0005) \n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=4,\n",
    "                                               gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1bd860-a88b-4f62-9fe3-4073cd5d013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10): # train with backbone now\n",
    "\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n",
    "    print (\"Train done, evaluating.\")\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    inference_res = evaluate(model,data_loader_val)\n",
    "    print('Inference done, computing mAp : ')\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.6, score_threshold = 0.05))\n",
    "    print('Epoch Done')\n",
    "    torch.save(model.state_dict(), '/app/host/lacmus/weights/resnet50_FRCNN_LADD_epoch_%i.pth'%epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e6d0f-88e9-4b3c-8b1e-f0095ea87e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment to test evaluation model and show detections\n",
    "\n",
    "dataset_test = LADDDataSET(voc_root,'test',get_transform(train=False,target_size=params['target_size'])) \n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=1\n",
    "     ,collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "image_idx = 0\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "model.eval()\n",
    "for images, targets in data_loader_test:\n",
    "    g_images = list(img.to(device) for img in images)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    outputs = model(g_images)\n",
    "\n",
    "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "    res = targets, outputs\n",
    "    break\n",
    "\n",
    "\n",
    "im = F.to_pil_image(images[image_idx])\n",
    "targets\n",
    "# im = to_pil_image(dataset[10][0])\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "for idx in range(len(outputs[image_idx]['boxes'])):\n",
    "    width = math.ceil(outputs[image_idx]['scores'][idx]*10)\n",
    "    bb = outputs[0]['boxes'][idx]\n",
    "    draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "               (bb[2], bb[1]), (bb[0], bb[1])], width=width, fill=(255, 0, 0))\n",
    "\n",
    "for bb in targets[image_idx]['boxes'][:10]:\n",
    "    draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "               (bb[2], bb[1]), (bb[0], bb[1])], width=4, fill=(0,255, 0))\n",
    "im.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa697837-d2e5-4466-b110-0eb143b0978d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
