{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f45ddd5-7de9-4bde-8a29-09a26d81c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.detection.retinanet import RetinaNet\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "import  torchvision.transforms.functional as F\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "from torchvision.datasets.voc import VisionDataset\n",
    "\n",
    "from functions import *\n",
    "from functions_torch import *\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd7e473-6220-4f7e-ac6b-c5736a3457e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['target_size']=(2000,1500)\n",
    "params['batch_size'] = 4\n",
    "params['lr'] = 0.001\n",
    "\n",
    "voc_root = '../../ladd-and-weights/dataset/full_train_ds'\n",
    "weights_root = '../../ladd-and-weights/weights/torch'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6f2ee2-33f3-4296-9e6e-4af7f364466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reworked class from pytorch (see https://pytorch.org/vision/0.8/_modules/torchvision/datasets/voc.html#VOCDetection)\n",
    "\n",
    "class LADDDataSET(torchvision.datasets.VisionDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            image_set: str,\n",
    "            transforms: Callable,\n",
    "            transforms_wo_norm: Callable\n",
    "            ):     \n",
    "        super(LADDDataSET, self).__init__(root, transforms=transforms)\n",
    "        self.image_set = image_set\n",
    "\n",
    "        voc_root = root\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "        if not os.path.isdir(voc_root):\n",
    "            raise RuntimeError('Dataset not found or corrupted.')\n",
    "\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets/Main')\n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        self.transforms_wo_norm = transforms_wo_norm\n",
    "        assert (len(self.images) == len(self.annotations))\n",
    "        \n",
    "    def get_data(self, index: int, transforms: Callable):\n",
    "        # Read an image with OpenCV\n",
    "        image = cv2.imread(self.images[index])\n",
    "        # By default OpenCV uses BGR color space for color images,\n",
    "        # so we need to convert the image to RGB color space.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        description = LADDDataSET.parse_voc_xml(\n",
    "            ET.parse(self.annotations[index]).getroot())\n",
    "\n",
    "        # get bounding box coordinates \n",
    "        num_objs = len(description['annotation']['object'])\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for l in description['annotation']['object']:\n",
    "            bb = l['bndbox']\n",
    "            boxes.append([int(bb['xmin']), int(bb['ymin']), int(bb['xmax']), int(bb['ymax'])]) \n",
    "            labels.append(1)\n",
    "        augmented = transforms(image=image, bboxes=boxes, labels=labels)\n",
    "        image = augmented['image']\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(augmented['bboxes'], dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(augmented['labels'],dtype=torch.int64)\n",
    "        \n",
    "        return image,target\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.get_data(index,self.transforms)\n",
    "        \n",
    "    \n",
    "    def get_wo_norm(self, index: int):        \n",
    "        return self.get_data(index, self.transforms_wo_norm)\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_voc_xml(node: ET.Element) -> Dict[str, Any]:\n",
    "        voc_dict: Dict[str, Any] = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
    "            for dc in map(LADDDataSET.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag == 'annotation':\n",
    "                def_dic['object'] = [def_dic['object']]\n",
    "            voc_dict = {\n",
    "                node.tag:\n",
    "                    {ind: v[0] if len(v) == 1 else v\n",
    "                     for ind, v in def_dic.items()}\n",
    "            }\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e17d1c-0cdf-4c95-9622-fba7f3022ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pytorch implemenation of retinanet doesn't supports train on Images without any objects (which, probably need to be fixed)\n",
    "# # see https://github.com/pytorch/vision/blob/master/torchvision/models/detection/retinanet.py#L475\n",
    "# # As a temporary solution, yet, we just filtering out empty images\n",
    "\n",
    "# splits_dir = os.path.join(voc_root, 'ImageSets/Main') \n",
    "# annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "# with open(os.path.join(splits_dir,'train.txt'), \"r\") as f:\n",
    "#     file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "# non_empty = []\n",
    "# for a in file_names:\n",
    "#     description = LADDDataSET.parse_voc_xml(\n",
    "#         ET.parse(os.path.join(annotation_dir, a + \".xml\")).getroot()\n",
    "#     )\n",
    "#     num_objs = len(description['annotation']['object'])\n",
    "#     if num_objs > 0:\n",
    "#         non_empty.append(a+'\\n')\n",
    "        \n",
    "# with open(os.path.join(splits_dir,'train_non_empty.txt'), \"w\") as f:\n",
    "#     f.writelines(non_empty)\n",
    "\n",
    "# print('Total images '+str(len(file_names)), ' non empty: '+str(len(non_empty)))\n",
    "                                                \n",
    "                                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad793266-f8c9-4c47-bc8d-32c1b1628e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "albumentations_transform_train = A.Compose([\n",
    "    A.Resize(params['target_size'][0],params['target_size'][1]), \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ChannelShuffle(p=0.5),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',label_fields=['labels']))\n",
    "\n",
    "\n",
    "albumentations_transform_val = A.Compose([\n",
    "    A.Resize(params['target_size'][0],params['target_size'][1]), \n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',label_fields=['labels']))\n",
    "\n",
    "albumentations_transform_train_view = A.Compose([\n",
    "    A.Resize(params['target_size'][0],params['target_size'][1]), \n",
    "    A.HorizontalFlip(p=1.0),\n",
    "    A.ChannelShuffle(p=1.0),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',label_fields=['labels']))\n",
    "\n",
    "albumentations_transform_val_view = A.Compose([\n",
    "    A.Resize(params['target_size'][0],params['target_size'][1]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc',label_fields=['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd78b6f-b80d-4b32-a00f-7e558ff2c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test DS\n",
    "im_idx = 99\n",
    "\n",
    "dataset = LADDDataSET(voc_root,'test',albumentations_transform_train,albumentations_transform_train_view) \n",
    "(image,target) = dataset.get_wo_norm(im_idx) \n",
    "im = F.to_pil_image(image)\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "for bb in target['boxes']:\n",
    "    draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "               (bb[2], bb[1]), (bb[0], bb[1])], width=4, fill=(255, 0, 0))\n",
    "\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502b74f2-3125-488e-8396-71806afa56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = LADDDataSET(voc_root,'train_non_empty',albumentations_transform_train,\n",
    "                           albumentations_transform_train_view) \n",
    "dataset_val = LADDDataSET(voc_root,'val',albumentations_transform_val, \n",
    "                          albumentations_transform_val_view) \n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=params['batch_size'], shuffle=True, num_workers=4\n",
    "     ,collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=16\n",
    "     ,collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a2b766-ffa4-4902-b3ac-6ae5cbc46b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_sizes = tuple((x, int(x * 2 ** (1.0 / 3)), int(x * 2 ** (2.0 / 3))) for x in [16, 32, 64, 128, 256])\n",
    "aspect_ratios = ((0.5, 1.0, 2.0, 3.0),) * len(anchor_sizes)\n",
    "anchor_generator = AnchorGenerator(\n",
    "    anchor_sizes, aspect_ratios\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8232f676-a422-4141-ae97-2ca0162a0426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "                                                           min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "                                                           trainable_backbone_layers = 0, anchor_generator=anchor_generator)\n",
    "# Nees to define pretrained_backbone to use trainable_backbone_layers, otherwise it's ignored\n",
    "model.load_state_dict(torch.load(os.path.join(weights_root,'pretrain','resnet50_SDD.pth')), strict=False)\n",
    "# model.load_state_dict(torch.load(os.path.join(weights_root,'pretrain','resnet50_SDD.pth'),\n",
    "#             map_location=torch.device('cpu')\n",
    "#             ), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ab91966-a90d-4336-8d91-8ac7f138631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "#                                                            min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "#                                                            trainable_backbone_layers = 0)\n",
    "# # Nees to define pretrained_backbone to use trainable_backbone_layers, otherwise it's ignored\n",
    "# model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_FRCNN_SDD_epoch_4.pth'), strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ada00ab-91d8-4522-9a03-994bc3e4f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computation device\n",
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9, weight_decay=0.0005) \n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e5f949-d04c-4427-9606-16794545f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/295]  eta: 0:11:47  lr: 0.000004  loss: 2.2853 (2.2853)  classification: 1.5119 (1.5119)  bbox_regression: 0.7735 (0.7735)  time: 2.3986  data: 1.2279  max mem: 3469\n",
      "Epoch: [0]  [100/295]  eta: 0:03:47  lr: 0.000344  loss: 1.4946 (1.8446)  classification: 0.8052 (1.1239)  bbox_regression: 0.6467 (0.7208)  time: 1.1675  data: 0.0234  max mem: 3547\n",
      "Epoch: [0]  [200/295]  eta: 0:01:50  lr: 0.000684  loss: 1.2573 (1.6019)  classification: 0.6739 (0.9456)  bbox_regression: 0.5788 (0.6563)  time: 1.1734  data: 0.0235  max mem: 3577\n",
      "Epoch: [0]  [294/295]  eta: 0:00:01  lr: 0.001000  loss: 1.2312 (1.4785)  classification: 0.6612 (0.8508)  bbox_regression: 0.5538 (0.6277)  time: 1.1752  data: 0.0240  max mem: 8164\n",
      "Epoch: [0] Total time: 0:05:45 (1.1712 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.21985391095081178, 0.026562345902232158)\n",
      "(0.0741508144717369, 0.016831585522206514)\n",
      "Epoch Done\n",
      "Epoch: [1]  [  0/295]  eta: 0:13:21  lr: 0.001000  loss: 1.6292 (1.6292)  classification: 1.0515 (1.0515)  bbox_regression: 0.5777 (0.5777)  time: 2.7159  data: 1.5292  max mem: 8164\n",
      "Epoch: [1]  [100/295]  eta: 0:03:49  lr: 0.001000  loss: 0.9972 (1.1200)  classification: 0.4894 (0.5797)  bbox_regression: 0.5217 (0.5403)  time: 1.1639  data: 0.0225  max mem: 8164\n",
      "Epoch: [1]  [200/295]  eta: 0:01:51  lr: 0.001000  loss: 1.0484 (1.1061)  classification: 0.4901 (0.5641)  bbox_regression: 0.5395 (0.5420)  time: 1.1634  data: 0.0223  max mem: 8164\n",
      "Epoch: [1]  [294/295]  eta: 0:00:01  lr: 0.001000  loss: 0.9748 (1.0953)  classification: 0.4789 (0.5578)  bbox_regression: 0.4944 (0.5374)  time: 1.1697  data: 0.0228  max mem: 8164\n",
      "Epoch: [1] Total time: 0:05:44 (1.1694 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.37931335787895204, 0.032012363395518266)\n",
      "(0.18785144872568227, 0.02178312543695036)\n",
      "Epoch Done\n",
      "Epoch: [2]  [  0/295]  eta: 0:12:23  lr: 0.001000  loss: 1.1972 (1.1972)  classification: 0.5851 (0.5851)  bbox_regression: 0.6121 (0.6121)  time: 2.5203  data: 1.3282  max mem: 8164\n",
      "Epoch: [2]  [100/295]  eta: 0:03:49  lr: 0.001000  loss: 1.0082 (1.0471)  classification: 0.5273 (0.5384)  bbox_regression: 0.4737 (0.5087)  time: 1.1621  data: 0.0227  max mem: 8164\n",
      "Epoch: [2]  [200/295]  eta: 0:01:51  lr: 0.001000  loss: 0.9762 (1.0042)  classification: 0.4894 (0.5050)  bbox_regression: 0.4948 (0.4992)  time: 1.1654  data: 0.0232  max mem: 8164\n",
      "Epoch: [2]  [294/295]  eta: 0:00:01  lr: 0.001000  loss: 1.0088 (0.9988)  classification: 0.5031 (0.5009)  bbox_regression: 0.5089 (0.4979)  time: 1.1626  data: 0.0222  max mem: 8164\n",
      "Epoch: [2] Total time: 0:05:45 (1.1715 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.40359979600640866, 0.1348426835358748)\n",
      "(0.23941265995628608, 0.09855168969535542)\n",
      "Epoch Done\n",
      "Epoch: [3]  [  0/295]  eta: 0:11:50  lr: 0.000100  loss: 0.9156 (0.9156)  classification: 0.4697 (0.4697)  bbox_regression: 0.4458 (0.4458)  time: 2.4082  data: 1.2223  max mem: 8164\n",
      "Epoch: [3]  [100/295]  eta: 0:03:51  lr: 0.000100  loss: 0.8660 (0.9322)  classification: 0.4058 (0.4591)  bbox_regression: 0.4584 (0.4731)  time: 1.1748  data: 0.0227  max mem: 8164\n",
      "Epoch: [3]  [200/295]  eta: 0:01:52  lr: 0.000100  loss: 0.8628 (0.9221)  classification: 0.4063 (0.4495)  bbox_regression: 0.4841 (0.4726)  time: 1.1751  data: 0.0233  max mem: 8164\n",
      "Epoch: [3]  [294/295]  eta: 0:00:01  lr: 0.000100  loss: 0.8819 (0.9199)  classification: 0.4109 (0.4502)  bbox_regression: 0.4764 (0.4697)  time: 1.1659  data: 0.0219  max mem: 8164\n",
      "Epoch: [3] Total time: 0:05:47 (1.1775 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.4659245477655264, 0.05107110422565308)\n",
      "(0.29968446572654345, 0.03792937656926118)\n",
      "Epoch Done\n",
      "Epoch: [4]  [  0/295]  eta: 0:12:31  lr: 0.000100  loss: 0.8386 (0.8386)  classification: 0.3901 (0.3901)  bbox_regression: 0.4485 (0.4485)  time: 2.5472  data: 1.3524  max mem: 8164\n",
      "Epoch: [4]  [100/295]  eta: 0:03:50  lr: 0.000100  loss: 0.8909 (0.8938)  classification: 0.4152 (0.4315)  bbox_regression: 0.4742 (0.4624)  time: 1.1746  data: 0.0226  max mem: 8164\n",
      "Epoch: [4]  [200/295]  eta: 0:01:51  lr: 0.000100  loss: 0.9197 (0.8909)  classification: 0.4451 (0.4314)  bbox_regression: 0.4413 (0.4596)  time: 1.1752  data: 0.0238  max mem: 8164\n",
      "Epoch: [4]  [294/295]  eta: 0:00:01  lr: 0.000100  loss: 0.8768 (0.8895)  classification: 0.4133 (0.4306)  bbox_regression: 0.4575 (0.4589)  time: 1.1711  data: 0.0237  max mem: 8164\n",
      "Epoch: [4] Total time: 0:05:47 (1.1778 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.4717650224069416, 0.048719110171624135)\n",
      "(0.3068392950442458, 0.03754592581408224)\n",
      "Epoch Done\n",
      "Epoch: [5]  [  0/295]  eta: 0:11:50  lr: 0.000100  loss: 0.7373 (0.7373)  classification: 0.3422 (0.3422)  bbox_regression: 0.3951 (0.3951)  time: 2.4094  data: 1.2111  max mem: 8164\n",
      "Epoch: [5]  [100/295]  eta: 0:03:49  lr: 0.000100  loss: 0.7584 (0.8820)  classification: 0.3605 (0.4264)  bbox_regression: 0.4205 (0.4556)  time: 1.1648  data: 0.0233  max mem: 8164\n",
      "Epoch: [5]  [200/295]  eta: 0:01:51  lr: 0.000100  loss: 0.8991 (0.8949)  classification: 0.4438 (0.4339)  bbox_regression: 0.4575 (0.4610)  time: 1.1647  data: 0.0229  max mem: 8174\n",
      "Epoch: [5]  [294/295]  eta: 0:00:01  lr: 0.000100  loss: 0.8046 (0.8853)  classification: 0.3789 (0.4293)  bbox_regression: 0.4180 (0.4560)  time: 1.1636  data: 0.0222  max mem: 8174\n",
      "Epoch: [5] Total time: 0:05:44 (1.1694 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.485443544955905, 0.04690295440259175)\n",
      "(0.30782308685643367, 0.03578163531744113)\n",
      "Epoch Done\n",
      "Epoch: [6]  [  0/295]  eta: 0:12:32  lr: 0.000010  loss: 1.0707 (1.0707)  classification: 0.5751 (0.5751)  bbox_regression: 0.4956 (0.4956)  time: 2.5509  data: 1.3645  max mem: 8174\n",
      "Epoch: [6]  [100/295]  eta: 0:03:49  lr: 0.000010  loss: 0.8806 (0.8836)  classification: 0.3794 (0.4302)  bbox_regression: 0.4311 (0.4534)  time: 1.1620  data: 0.0220  max mem: 8174\n",
      "Epoch: [6]  [200/295]  eta: 0:01:51  lr: 0.000010  loss: 0.8618 (0.8831)  classification: 0.4099 (0.4295)  bbox_regression: 0.4297 (0.4536)  time: 1.1644  data: 0.0222  max mem: 8174\n",
      "Epoch: [6]  [294/295]  eta: 0:00:01  lr: 0.000010  loss: 0.8594 (0.8729)  classification: 0.4341 (0.4220)  bbox_regression: 0.4559 (0.4509)  time: 1.1630  data: 0.0217  max mem: 8174\n",
      "Epoch: [6] Total time: 0:05:44 (1.1691 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.48641686757140534, 0.054945679995449644)\n",
      "(0.31344525872140916, 0.04197713440646152)\n",
      "Epoch Done\n",
      "Epoch: [7]  [  0/295]  eta: 0:12:07  lr: 0.000010  loss: 0.9433 (0.9433)  classification: 0.4369 (0.4369)  bbox_regression: 0.5065 (0.5065)  time: 2.4647  data: 1.2503  max mem: 8174\n",
      "Epoch: [7]  [100/295]  eta: 0:03:50  lr: 0.000010  loss: 0.8027 (0.8756)  classification: 0.3880 (0.4190)  bbox_regression: 0.4336 (0.4566)  time: 1.1774  data: 0.0239  max mem: 8174\n",
      "Epoch: [7]  [200/295]  eta: 0:01:51  lr: 0.000010  loss: 0.8030 (0.8830)  classification: 0.3343 (0.4261)  bbox_regression: 0.4574 (0.4568)  time: 1.1708  data: 0.0231  max mem: 8174\n",
      "Epoch: [7]  [294/295]  eta: 0:00:01  lr: 0.000010  loss: 0.8585 (0.8761)  classification: 0.4012 (0.4221)  bbox_regression: 0.4469 (0.4540)  time: 1.1738  data: 0.0229  max mem: 8174\n",
      "Epoch: [7] Total time: 0:05:46 (1.1754 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.48617179750187756, 0.05471020687296973)\n",
      "(0.31388009920261273, 0.041830512338291444)\n",
      "Epoch Done\n",
      "Epoch: [8]  [  0/295]  eta: 0:12:28  lr: 0.000010  loss: 0.7168 (0.7168)  classification: 0.3391 (0.3391)  bbox_regression: 0.3777 (0.3777)  time: 2.5378  data: 1.3457  max mem: 8174\n",
      "Epoch: [8]  [100/295]  eta: 0:03:49  lr: 0.000010  loss: 0.8173 (0.8644)  classification: 0.3588 (0.4137)  bbox_regression: 0.4049 (0.4507)  time: 1.1661  data: 0.0231  max mem: 8174\n",
      "Epoch: [8]  [200/295]  eta: 0:01:51  lr: 0.000010  loss: 0.8431 (0.8595)  classification: 0.3923 (0.4136)  bbox_regression: 0.4558 (0.4459)  time: 1.1662  data: 0.0223  max mem: 8174\n",
      "Epoch: [8]  [294/295]  eta: 0:00:01  lr: 0.000010  loss: 0.8261 (0.8648)  classification: 0.3814 (0.4144)  bbox_regression: 0.4459 (0.4504)  time: 1.1669  data: 0.0225  max mem: 8174\n",
      "Epoch: [8] Total time: 0:05:45 (1.1700 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.4879647499381454, 0.05262583251446665)\n",
      "(0.3059527061715019, 0.04061578774975434)\n",
      "Epoch Done\n",
      "Epoch: [9]  [  0/295]  eta: 0:12:36  lr: 0.000001  loss: 0.9198 (0.9198)  classification: 0.4640 (0.4640)  bbox_regression: 0.4559 (0.4559)  time: 2.5657  data: 1.3791  max mem: 8174\n",
      "Epoch: [9]  [100/295]  eta: 0:03:50  lr: 0.000001  loss: 0.8166 (0.8903)  classification: 0.3906 (0.4362)  bbox_regression: 0.4339 (0.4541)  time: 1.1654  data: 0.0230  max mem: 8174\n",
      "Epoch: [9]  [200/295]  eta: 0:01:51  lr: 0.000001  loss: 0.8083 (0.8736)  classification: 0.3581 (0.4224)  bbox_regression: 0.4415 (0.4512)  time: 1.1669  data: 0.0242  max mem: 8174\n",
      "Epoch: [9]  [294/295]  eta: 0:00:01  lr: 0.000001  loss: 0.7965 (0.8684)  classification: 0.3688 (0.4195)  bbox_regression: 0.4252 (0.4489)  time: 1.1658  data: 0.0226  max mem: 8174\n",
      "Epoch: [9] Total time: 0:05:45 (1.1714 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.49032946881897055, 0.05372500837708031)\n",
      "(0.31194032979669817, 0.04143862392494137)\n",
      "Epoch Done\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10): # train without backbone\n",
    "\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n",
    "    print (\"Train done, evaluating.\")\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    inference_res = evaluate(model,data_loader_val)\n",
    "    print('Inference done, computing mAp : ')\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.6, score_threshold = 0.05))\n",
    "    print('Epoch Done')\n",
    "torch.save(model.state_dict(), 'resnet50_RetinaNet_LADD_head.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df264737-5016-48af-9036-b3ad68ca73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "del optimizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28c30f30-5058-413c-ac00-5915a2ebfec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_sizes = tuple((x, int(x * 2 ** (1.0 / 3)), int(x * 2 ** (2.0 / 3))) for x in [16, 32, 64, 128, 256])\n",
    "aspect_ratios = ((0.5, 1.0, 2.0, 3.0),) * len(anchor_sizes)\n",
    "anchor_generator = AnchorGenerator(\n",
    "    anchor_sizes, aspect_ratios\n",
    ")\n",
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "                                                           min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "                                                           trainable_backbone_layers = 5, anchor_generator=anchor_generator)\n",
    "model.load_state_dict(torch.load('resnet50_RetinaNet_LADD_head.pth'), strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89059b62-58cb-415a-acd9-2a5c15a4845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True, \n",
    "#                                                            min_size=params['target_size'][0], max_size = params['target_size'][1],\n",
    "#                                                            trainable_backbone_layers = 5)\n",
    "# model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_FRCNN_LADD_head.pth'), strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b18ba868-2f71-4220-a400-940a220eae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computation device\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9, weight_decay=0.0005) \n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=4,\n",
    "                                               gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df1bd860-a88b-4f62-9fe3-4073cd5d013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/295]  eta: 0:17:08  lr: 0.000004  loss: 0.5877 (0.5877)  classification: 0.2064 (0.2064)  bbox_regression: 0.3813 (0.3813)  time: 3.4848  data: 1.5528  max mem: 8543\n",
      "Epoch: [0]  [100/295]  eta: 0:06:13  lr: 0.000344  loss: 0.9916 (0.9058)  classification: 0.5213 (0.4497)  bbox_regression: 0.4703 (0.4561)  time: 1.9013  data: 0.0215  max mem: 13375\n",
      "Epoch: [0]  [200/295]  eta: 0:03:01  lr: 0.000684  loss: 0.7527 (0.8682)  classification: 0.3347 (0.4161)  bbox_regression: 0.4344 (0.4521)  time: 1.9040  data: 0.0218  max mem: 13375\n",
      "Epoch: [0]  [294/295]  eta: 0:00:01  lr: 0.001000  loss: 0.7914 (0.8501)  classification: 0.3440 (0.4005)  bbox_regression: 0.4364 (0.4496)  time: 1.9008  data: 0.0212  max mem: 13375\n",
      "Epoch: [0] Total time: 0:09:22 (1.9080 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.6454312257489907, 0.06352670182457416)\n",
      "(0.47089293577780883, 0.05322179790264897)\n",
      "Epoch Done\n",
      "Epoch: [1]  [  0/295]  eta: 0:17:18  lr: 0.001000  loss: 0.7814 (0.7814)  classification: 0.3212 (0.3212)  bbox_regression: 0.4602 (0.4602)  time: 3.5214  data: 1.5952  max mem: 13375\n",
      "Epoch: [1]  [100/295]  eta: 0:06:14  lr: 0.001000  loss: 0.7965 (0.7783)  classification: 0.3725 (0.3473)  bbox_regression: 0.4089 (0.4310)  time: 1.9027  data: 0.0215  max mem: 13375\n",
      "Epoch: [1]  [200/295]  eta: 0:03:01  lr: 0.001000  loss: 0.7697 (0.7579)  classification: 0.3345 (0.3343)  bbox_regression: 0.4160 (0.4236)  time: 1.9073  data: 0.0219  max mem: 13375\n",
      "Epoch: [1]  [294/295]  eta: 0:00:01  lr: 0.001000  loss: 0.6820 (0.7319)  classification: 0.2360 (0.3173)  bbox_regression: 0.4079 (0.4146)  time: 1.8999  data: 0.0209  max mem: 13375\n",
      "Epoch: [1] Total time: 0:09:23 (1.9093 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.6958809996149181, 0.1925454204907286)\n",
      "(0.5251886574295666, 0.16707248548417308)\n",
      "Epoch Done\n",
      "Epoch: [2]  [  0/295]  eta: 0:16:11  lr: 0.001000  loss: 0.7655 (0.7655)  classification: 0.3442 (0.3442)  bbox_regression: 0.4213 (0.4213)  time: 3.2922  data: 1.3298  max mem: 13375\n",
      "Epoch: [2]  [100/295]  eta: 0:06:13  lr: 0.001000  loss: 0.6317 (0.6687)  classification: 0.2636 (0.2788)  bbox_regression: 0.3722 (0.3898)  time: 1.9014  data: 0.0212  max mem: 13375\n",
      "Epoch: [2]  [200/295]  eta: 0:03:01  lr: 0.001000  loss: 0.6860 (0.6758)  classification: 0.3176 (0.2848)  bbox_regression: 0.3780 (0.3911)  time: 1.9045  data: 0.0218  max mem: 13375\n",
      "Epoch: [2]  [294/295]  eta: 0:00:01  lr: 0.001000  loss: 0.5860 (0.6562)  classification: 0.2209 (0.2736)  bbox_regression: 0.3701 (0.3826)  time: 1.9107  data: 0.0222  max mem: 13375\n",
      "Epoch: [2] Total time: 0:09:23 (1.9091 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.7611091064687379, 0.1634301693335405)\n",
      "(0.5999660824490052, 0.14665216715861426)\n",
      "Epoch Done\n",
      "Epoch: [3]  [  0/295]  eta: 0:16:22  lr: 0.001000  loss: 0.6993 (0.6993)  classification: 0.3046 (0.3046)  bbox_regression: 0.3947 (0.3947)  time: 3.3298  data: 1.3631  max mem: 13375\n",
      "Epoch: [3]  [100/295]  eta: 0:06:14  lr: 0.001000  loss: 0.5471 (0.5895)  classification: 0.2158 (0.2327)  bbox_regression: 0.3419 (0.3569)  time: 1.9009  data: 0.0214  max mem: 13375\n",
      "Epoch: [3]  [200/295]  eta: 0:03:01  lr: 0.001000  loss: 0.5595 (0.5988)  classification: 0.1975 (0.2403)  bbox_regression: 0.3425 (0.3585)  time: 1.9108  data: 0.0223  max mem: 13375\n",
      "Epoch: [3]  [294/295]  eta: 0:00:01  lr: 0.001000  loss: 0.5767 (0.5935)  classification: 0.2027 (0.2346)  bbox_regression: 0.3496 (0.3589)  time: 1.9175  data: 0.0219  max mem: 13375\n",
      "Epoch: [3] Total time: 0:09:24 (1.9138 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.8153164226682335, 0.15791795324217028)\n",
      "(0.6890502119784495, 0.1455668284075871)\n",
      "Epoch Done\n",
      "Epoch: [4]  [  0/295]  eta: 0:15:42  lr: 0.000100  loss: 0.3676 (0.3676)  classification: 0.1107 (0.1107)  bbox_regression: 0.2570 (0.2570)  time: 3.1963  data: 1.2552  max mem: 13375\n",
      "Epoch: [4]  [100/295]  eta: 0:06:14  lr: 0.000100  loss: 0.5366 (0.5392)  classification: 0.2061 (0.2080)  bbox_regression: 0.3234 (0.3312)  time: 1.9050  data: 0.0222  max mem: 13375\n",
      "Epoch: [4]  [200/295]  eta: 0:03:01  lr: 0.000100  loss: 0.4432 (0.5207)  classification: 0.1487 (0.2016)  bbox_regression: 0.2944 (0.3191)  time: 1.9038  data: 0.0224  max mem: 13375\n",
      "Epoch: [4]  [294/295]  eta: 0:00:01  lr: 0.000100  loss: 0.4722 (0.5090)  classification: 0.1705 (0.1956)  bbox_regression: 0.2878 (0.3134)  time: 1.9034  data: 0.0217  max mem: 13375\n",
      "Epoch: [4] Total time: 0:09:23 (1.9097 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.8415672659157325, 0.24942156409069877)\n",
      "(0.7302906629460986, 0.2309116149930588)\n",
      "Epoch Done\n",
      "Epoch: [5]  [  0/295]  eta: 0:17:27  lr: 0.000100  loss: 0.4656 (0.4656)  classification: 0.1489 (0.1489)  bbox_regression: 0.3167 (0.3167)  time: 3.5514  data: 1.6109  max mem: 13375\n",
      "Epoch: [5]  [100/295]  eta: 0:06:14  lr: 0.000100  loss: 0.4761 (0.4876)  classification: 0.1884 (0.1850)  bbox_regression: 0.3075 (0.3026)  time: 1.9075  data: 0.0222  max mem: 13375\n",
      "Epoch: [5]  [200/295]  eta: 0:03:01  lr: 0.000100  loss: 0.4603 (0.4788)  classification: 0.1660 (0.1812)  bbox_regression: 0.2943 (0.2976)  time: 1.9115  data: 0.0223  max mem: 13375\n",
      "Epoch: [5]  [294/295]  eta: 0:00:01  lr: 0.000100  loss: 0.4817 (0.4875)  classification: 0.1728 (0.1872)  bbox_regression: 0.2908 (0.3002)  time: 1.9129  data: 0.0224  max mem: 13375\n",
      "Epoch: [5] Total time: 0:09:24 (1.9136 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.8455892480362823, 0.2270450751252087)\n",
      "(0.7408071282621397, 0.2099332220367279)\n",
      "Epoch Done\n",
      "Epoch: [6]  [  0/295]  eta: 0:16:55  lr: 0.000100  loss: 0.4381 (0.4381)  classification: 0.2041 (0.2041)  bbox_regression: 0.2340 (0.2340)  time: 3.4415  data: 1.4853  max mem: 13375\n",
      "Epoch: [6]  [100/295]  eta: 0:06:14  lr: 0.000100  loss: 0.4748 (0.4696)  classification: 0.1675 (0.1784)  bbox_regression: 0.3060 (0.2912)  time: 1.9036  data: 0.0216  max mem: 13375\n",
      "Epoch: [6]  [200/295]  eta: 0:03:01  lr: 0.000100  loss: 0.4231 (0.4662)  classification: 0.1657 (0.1772)  bbox_regression: 0.2890 (0.2889)  time: 1.9093  data: 0.0222  max mem: 13375\n",
      "Epoch: [6]  [294/295]  eta: 0:00:01  lr: 0.000100  loss: 0.4416 (0.4743)  classification: 0.1467 (0.1807)  bbox_regression: 0.3112 (0.2936)  time: 1.9051  data: 0.0217  max mem: 13375\n",
      "Epoch: [6] Total time: 0:09:23 (1.9115 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.8470775294386018, 0.25648346419224366)\n",
      "(0.7367136285935088, 0.23554603854389725)\n",
      "Epoch Done\n",
      "Epoch: [7]  [  0/295]  eta: 0:15:02  lr: 0.000100  loss: 0.5429 (0.5429)  classification: 0.1905 (0.1905)  bbox_regression: 0.3524 (0.3524)  time: 3.0586  data: 1.1822  max mem: 13375\n",
      "Epoch: [7]  [100/295]  eta: 0:06:13  lr: 0.000100  loss: 0.4800 (0.4738)  classification: 0.1802 (0.1825)  bbox_regression: 0.2938 (0.2913)  time: 1.9028  data: 0.0217  max mem: 13375\n",
      "Epoch: [7]  [200/295]  eta: 0:03:01  lr: 0.000100  loss: 0.4163 (0.4668)  classification: 0.1321 (0.1777)  bbox_regression: 0.2556 (0.2891)  time: 1.9043  data: 0.0216  max mem: 13375\n",
      "Epoch: [7]  [294/295]  eta: 0:00:01  lr: 0.000100  loss: 0.4108 (0.4648)  classification: 0.1449 (0.1762)  bbox_regression: 0.2594 (0.2886)  time: 1.9034  data: 0.0215  max mem: 13375\n",
      "Epoch: [7] Total time: 0:09:23 (1.9085 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.8513169754912653, 0.24810823205686774)\n",
      "(0.7442595695546901, 0.22838798440724606)\n",
      "Epoch Done\n",
      "Epoch: [8]  [  0/295]  eta: 0:15:42  lr: 0.000010  loss: 0.4876 (0.4876)  classification: 0.1716 (0.1716)  bbox_regression: 0.3161 (0.3161)  time: 3.1936  data: 1.2539  max mem: 13375\n",
      "Epoch: [8]  [100/295]  eta: 0:06:14  lr: 0.000010  loss: 0.4327 (0.4502)  classification: 0.1534 (0.1758)  bbox_regression: 0.2719 (0.2744)  time: 1.9050  data: 0.0215  max mem: 13375\n",
      "Epoch: [8]  [200/295]  eta: 0:03:01  lr: 0.000010  loss: 0.4657 (0.4525)  classification: 0.1553 (0.1733)  bbox_regression: 0.3023 (0.2792)  time: 1.9129  data: 0.0223  max mem: 13375\n",
      "Epoch: [8]  [294/295]  eta: 0:00:01  lr: 0.000010  loss: 0.4033 (0.4530)  classification: 0.1522 (0.1731)  bbox_regression: 0.2607 (0.2799)  time: 1.9024  data: 0.0219  max mem: 13375\n",
      "Epoch: [8] Total time: 0:09:24 (1.9130 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.8523904975447173, 0.2653865091088134)\n",
      "(0.7475446665877786, 0.24569177744953222)\n",
      "Epoch Done\n",
      "Epoch: [9]  [  0/295]  eta: 0:15:39  lr: 0.000010  loss: 0.3911 (0.3911)  classification: 0.1142 (0.1142)  bbox_regression: 0.2769 (0.2769)  time: 3.1851  data: 1.2480  max mem: 13375\n",
      "Epoch: [9]  [100/295]  eta: 0:06:13  lr: 0.000010  loss: 0.4605 (0.4488)  classification: 0.1696 (0.1725)  bbox_regression: 0.2858 (0.2763)  time: 1.9037  data: 0.0217  max mem: 13375\n",
      "Epoch: [9]  [200/295]  eta: 0:03:01  lr: 0.000010  loss: 0.4703 (0.4561)  classification: 0.1772 (0.1754)  bbox_regression: 0.2902 (0.2807)  time: 1.9105  data: 0.0218  max mem: 13375\n",
      "Epoch: [9]  [294/295]  eta: 0:00:01  lr: 0.000010  loss: 0.4069 (0.4508)  classification: 0.1377 (0.1723)  bbox_regression: 0.2692 (0.2785)  time: 1.9036  data: 0.0216  max mem: 13375\n",
      "Epoch: [9] Total time: 0:09:23 (1.9108 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.8546527215649026, 0.2681592039800995)\n",
      "(0.7420007161577482, 0.24676616915422886)\n",
      "Epoch Done\n",
      "Epoch: [10]  [  0/295]  eta: 0:15:42  lr: 0.000010  loss: 0.4990 (0.4990)  classification: 0.1867 (0.1867)  bbox_regression: 0.3123 (0.3123)  time: 3.1945  data: 1.2633  max mem: 13375\n",
      "Epoch: [10]  [100/295]  eta: 0:06:14  lr: 0.000010  loss: 0.4027 (0.4394)  classification: 0.1320 (0.1650)  bbox_regression: 0.2706 (0.2745)  time: 1.9121  data: 0.0224  max mem: 13375\n",
      "Epoch: [10]  [200/295]  eta: 0:03:01  lr: 0.000010  loss: 0.4101 (0.4418)  classification: 0.1488 (0.1673)  bbox_regression: 0.2565 (0.2746)  time: 1.9046  data: 0.0218  max mem: 13375\n",
      "Epoch: [10]  [294/295]  eta: 0:00:01  lr: 0.000010  loss: 0.3958 (0.4468)  classification: 0.1456 (0.1687)  bbox_regression: 0.2577 (0.2780)  time: 1.9048  data: 0.0218  max mem: 13375\n",
      "Epoch: [10] Total time: 0:09:23 (1.9119 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.8553048864659609, 0.27002745195907163)\n",
      "(0.7466710020412008, 0.24956326428749687)\n",
      "Epoch Done\n",
      "Epoch: [11]  [  0/295]  eta: 0:16:17  lr: 0.000010  loss: 0.3952 (0.3952)  classification: 0.1302 (0.1302)  bbox_regression: 0.2650 (0.2650)  time: 3.3139  data: 1.3693  max mem: 13375\n",
      "Epoch: [11]  [100/295]  eta: 0:06:14  lr: 0.000010  loss: 0.3932 (0.4433)  classification: 0.1364 (0.1690)  bbox_regression: 0.2568 (0.2743)  time: 1.9045  data: 0.0216  max mem: 13375\n",
      "Epoch: [11]  [200/295]  eta: 0:03:01  lr: 0.000010  loss: 0.4247 (0.4470)  classification: 0.1623 (0.1705)  bbox_regression: 0.2850 (0.2765)  time: 1.9037  data: 0.0217  max mem: 13375\n",
      "Epoch: [11]  [294/295]  eta: 0:00:01  lr: 0.000010  loss: 0.4452 (0.4469)  classification: 0.1399 (0.1689)  bbox_regression: 0.2950 (0.2780)  time: 1.9113  data: 0.0223  max mem: 13375\n",
      "Epoch: [11] Total time: 0:09:23 (1.9107 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "(0.8556192881314308, 0.278612118073537)\n",
      "(0.7460097223311024, 0.25634386328327297)\n",
      "Epoch Done\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(12): # train with backbone now\n",
    "\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n",
    "    print (\"Train done, evaluating.\")\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    inference_res = evaluate(model,data_loader_val)\n",
    "    print('Inference done, computing mAp : ')\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.6, score_threshold = 0.05))\n",
    "    print('Epoch Done')\n",
    "    torch.save(model.state_dict(), 'resnet50_RN_LADD_epoch_%i.pth'%(epoch+10))\n",
    "\n",
    "# was    \n",
    "# Epoch: [9] Total time: 0:09:23 (0.4774 s / it)\n",
    "# Train done, evaluating.\n",
    "# Inference done, computing mAp : \n",
    "# 0.8940549518273289\n",
    "# 0.8334125499913912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e6d0f-88e9-4b3c-8b1e-f0095ea87e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # uncomment to test evaluation model and show detections\n",
    "\n",
    "# dataset_test = LADDDataSET(voc_root,'test',albumentations_transform_val, albumentations_transform_val_view) \n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_val, batch_size=1, shuffle=False, num_workers=1\n",
    "#      ,collate_fn=collate_fn\n",
    "# )\n",
    "\n",
    "# image_idx = 0\n",
    "\n",
    "# cpu_device = torch.device(\"cpu\")\n",
    "# model.eval()\n",
    "# for images, targets in data_loader_test:\n",
    "#     g_images = list(img.to(device) for img in images)\n",
    "\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.synchronize()\n",
    "#     outputs = model(g_images)\n",
    "\n",
    "#     outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "#     res = targets, outputs\n",
    "#     break\n",
    "\n",
    "# (image,target) = dataset_test.get_wo_norm(im_idx) \n",
    "# im = F.to_pil_image(image)\n",
    "# draw = ImageDraw.Draw(im)\n",
    "\n",
    "# for idx in range(len(outputs[image_idx]['boxes'])):\n",
    "#     width = math.ceil(outputs[image_idx]['scores'][idx]*10)\n",
    "#     bb = outputs[0]['boxes'][idx]\n",
    "#     draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "#                (bb[2], bb[1]), (bb[0], bb[1])], width=width, fill=(255, 0, 0))\n",
    "\n",
    "# for bb in targets[image_idx]['boxes'][:10]:\n",
    "#     draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "#                (bb[2], bb[1]), (bb[0], bb[1])], width=4, fill=(0,255, 0))\n",
    "# im.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa697837-d2e5-4466-b110-0eb143b0978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img =  Image.open('..\\').convert('RGB')\n",
    "# g_images = list(img.to(device) for img in images)\n",
    "\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.synchronize()\n",
    "#     outputs = model(g_images)\n",
    "\n",
    "#     outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "#     res = targets, outputs\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eac9a6-5282-41ec-9aca-55df3a56b0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
