{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f45ddd5-7de9-4bde-8a29-09a26d81c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.detection.retinanet import RetinaNet\n",
    "import  torchvision.transforms.functional as F\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections\n",
    "from torchvision.datasets.voc import VisionDataset\n",
    "\n",
    "from functions import *\n",
    "from functions_torch import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd7e473-6220-4f7e-ac6b-c5736a3457e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['target_size']=(2000,1500)\n",
    "params['batch_size'] = 1\n",
    "params['lr'] = 0.001\n",
    "\n",
    "voc_root = '/app/host/lacmus/dataset/full_lacmus_ds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6f2ee2-33f3-4296-9e6e-4af7f364466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reworked class from pytorch (see https://pytorch.org/vision/0.8/_modules/torchvision/datasets/voc.html#VOCDetection)\n",
    "\n",
    "class LADDDataSET(torchvision.datasets.VisionDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            image_set: str,\n",
    "            transforms: Optional[Callable] = None):     \n",
    "        super(LADDDataSET, self).__init__(root, transforms=transforms)\n",
    "        self.image_set = image_set\n",
    "\n",
    "        voc_root = root\n",
    "        image_dir = os.path.join(voc_root, 'JPEGImages')\n",
    "        annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "        if not os.path.isdir(voc_root):\n",
    "            raise RuntimeError('Dataset not found or corrupted.')\n",
    "\n",
    "        splits_dir = os.path.join(voc_root, 'ImageSets/Main')\n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip('\\n') + '.txt')\n",
    "\n",
    "        with open(os.path.join(split_f), \"r\") as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
    "        self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
    "        assert (len(self.images) == len(self.annotations))\n",
    "        \n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a dictionary of the XML tree.\n",
    "        \"\"\"\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        description = LADDDataSET.parse_voc_xml(\n",
    "            ET.parse(self.annotations[index]).getroot())\n",
    "\n",
    "        # get bounding box coordinates \n",
    "        num_objs = len(description['annotation']['object'])\n",
    "        boxes = []\n",
    "        for l in description['annotation']['object']:\n",
    "            bb = l['bndbox']\n",
    "            boxes.append([int(bb['xmin']), int(bb['ymin']), int(bb['xmax']), int(bb['ymax'])])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)         # there is only one class            \n",
    "        target[\"labels\"] = labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_voc_xml(node: ET.Element) -> Dict[str, Any]:\n",
    "        voc_dict: Dict[str, Any] = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
    "            for dc in map(LADDDataSET.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag == 'annotation':\n",
    "                def_dic['object'] = [def_dic['object']]\n",
    "            voc_dict = {\n",
    "                node.tag:\n",
    "                    {ind: v[0] if len(v) == 1 else v\n",
    "                     for ind, v in def_dic.items()}\n",
    "            }\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e17d1c-0cdf-4c95-9622-fba7f3022ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images 1220  non empty: 1180\n"
     ]
    }
   ],
   "source": [
    "# Pytorch implemenation of retinanet doesn't supports train on Images without any objects (which, probably need to be fixed)\n",
    "# see https://github.com/pytorch/vision/blob/master/torchvision/models/detection/retinanet.py#L475\n",
    "# As a temporary solution, yet, we just filtering out empty images\n",
    "\n",
    "splits_dir = os.path.join(voc_root, 'ImageSets/Main') \n",
    "annotation_dir = os.path.join(voc_root, 'Annotations')\n",
    "\n",
    "with open(os.path.join(splits_dir,'train.txt'), \"r\") as f:\n",
    "    file_names = [x.strip() for x in f.readlines()]\n",
    "\n",
    "non_empty = []\n",
    "for a in file_names:\n",
    "    description = LADDDataSET.parse_voc_xml(\n",
    "        ET.parse(os.path.join(annotation_dir, a + \".xml\")).getroot()\n",
    "    )\n",
    "    num_objs = len(description['annotation']['object'])\n",
    "    if num_objs > 0:\n",
    "        non_empty.append(a+'\\n')\n",
    "        \n",
    "with open(os.path.join(splits_dir,'train_non_empty.txt'), \"w\") as f:\n",
    "    f.writelines(non_empty)\n",
    "\n",
    "print('Total images '+str(len(file_names)), ' non empty: '+str(len(non_empty)))\n",
    "                                                \n",
    "                                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd78b6f-b80d-4b32-a00f-7e558ff2c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test DS\n",
    "# im_idx = 99\n",
    "\n",
    "# dataset = LADDDataSET('/app/host/lacmus/dataset/full_lacmus_ds','test',get_transform(train=True,target_size=params['target_size'])) \n",
    "# (image,target) = dataset[im_idx] \n",
    "# im = F.to_pil_image(image)\n",
    "# draw = ImageDraw.Draw(im)\n",
    "\n",
    "# for bb in target['boxes']:\n",
    "#     draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "#                (bb[2], bb[1]), (bb[0], bb[1])], width=4, fill=(255, 0, 0))\n",
    "\n",
    "# im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "502b74f2-3125-488e-8396-71806afa56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = LADDDataSET(voc_root,'train_non_empty',get_transform(train=True,target_size=params['target_size'])) \n",
    "dataset_val = LADDDataSET(voc_root,'val',get_transform(train=False,target_size=params['target_size'])) \n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=params['batch_size'], shuffle=True, num_workers=4\n",
    "     ,collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=4\n",
    "     ,collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8232f676-a422-4141-ae97-2ca0162a0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=False, \n",
    "                                                           min_size=params['target_size'][0], max_size = params['target_size'][1])\n",
    "model.load_state_dict(torch.load('/app/host/lacmus/weights/resnet50_SDD_epoch_8.pth'), strict=False)\n",
    "\n",
    "# the computation device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9, weight_decay=0.0005) #lr 0.001 -> 0.005\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5f949-d04c-4427-9606-16794545f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/1180]  eta: 0:25:37  lr: 0.000002  loss: 0.9846 (0.9846)  classification: 0.4986 (0.4986)  bbox_regression: 0.4859 (0.4859)  time: 1.3034  data: 0.8602  max mem: 2243\n",
      "Epoch: [0]  [ 100/1180]  eta: 0:08:11  lr: 0.000102  loss: 1.1960 (1.5684)  classification: 0.5574 (0.7180)  bbox_regression: 0.6622 (0.8504)  time: 0.4471  data: 0.0094  max mem: 2532\n",
      "Epoch: [0]  [ 200/1180]  eta: 0:07:23  lr: 0.000202  loss: 1.1922 (1.3587)  classification: 0.4271 (0.6163)  bbox_regression: 0.6627 (0.7424)  time: 0.4540  data: 0.0114  max mem: 2532\n",
      "Epoch: [0]  [ 300/1180]  eta: 0:06:41  lr: 0.000302  loss: 0.8619 (1.2302)  classification: 0.3754 (0.5530)  bbox_regression: 0.4774 (0.6772)  time: 0.4637  data: 0.0118  max mem: 2532\n",
      "Epoch: [0]  [ 400/1180]  eta: 0:05:57  lr: 0.000402  loss: 0.9241 (1.1860)  classification: 0.3910 (0.5190)  bbox_regression: 0.5610 (0.6670)  time: 0.4643  data: 0.0120  max mem: 2532\n",
      "Epoch: [0]  [ 500/1180]  eta: 0:05:12  lr: 0.000501  loss: 0.8291 (1.1342)  classification: 0.3386 (0.4933)  bbox_regression: 0.5256 (0.6409)  time: 0.4636  data: 0.0123  max mem: 2532\n",
      "Epoch: [0]  [ 600/1180]  eta: 0:04:26  lr: 0.000601  loss: 0.8533 (1.1050)  classification: 0.3138 (0.4767)  bbox_regression: 0.4763 (0.6283)  time: 0.4606  data: 0.0098  max mem: 4304\n",
      "Epoch: [0]  [ 700/1180]  eta: 0:03:40  lr: 0.000701  loss: 0.8939 (1.0863)  classification: 0.3143 (0.4668)  bbox_regression: 0.5091 (0.6195)  time: 0.4622  data: 0.0106  max mem: 4304\n",
      "Epoch: [0]  [ 800/1180]  eta: 0:02:54  lr: 0.000801  loss: 0.9706 (1.0632)  classification: 0.4551 (0.4534)  bbox_regression: 0.4972 (0.6098)  time: 0.4637  data: 0.0104  max mem: 4304\n",
      "Epoch: [0]  [ 900/1180]  eta: 0:02:08  lr: 0.000901  loss: 0.8072 (1.0406)  classification: 0.3268 (0.4411)  bbox_regression: 0.4754 (0.5996)  time: 0.4615  data: 0.0093  max mem: 4304\n",
      "Epoch: [0]  [1000/1180]  eta: 0:01:22  lr: 0.001000  loss: 0.7522 (1.0285)  classification: 0.3243 (0.4347)  bbox_regression: 0.4827 (0.5938)  time: 0.4616  data: 0.0101  max mem: 5999\n",
      "Epoch: [0]  [1100/1180]  eta: 0:00:36  lr: 0.001000  loss: 0.8101 (1.0105)  classification: 0.2879 (0.4268)  bbox_regression: 0.5041 (0.5837)  time: 0.4614  data: 0.0106  max mem: 5999\n",
      "Epoch: [0]  [1179/1180]  eta: 0:00:00  lr: 0.001000  loss: 0.8299 (0.9972)  classification: 0.2569 (0.4200)  bbox_regression: 0.4838 (0.5772)  time: 0.4617  data: 0.0103  max mem: 5999\n",
      "Epoch: [0] Total time: 0:09:03 (0.4608 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "0.4914366726176926\n",
      "0.26113478368152554\n",
      "Epoch Done\n",
      "Epoch: [1]  [   0/1180]  eta: 0:31:45  lr: 0.001000  loss: 1.0755 (1.0755)  classification: 0.4042 (0.4042)  bbox_regression: 0.6713 (0.6713)  time: 1.6144  data: 1.1404  max mem: 5999\n",
      "Epoch: [1]  [ 100/1180]  eta: 0:08:30  lr: 0.001000  loss: 0.6297 (0.7785)  classification: 0.2354 (0.3265)  bbox_regression: 0.4064 (0.4520)  time: 0.4622  data: 0.0100  max mem: 5999\n",
      "Epoch: [1]  [ 200/1180]  eta: 0:07:37  lr: 0.001000  loss: 0.8229 (0.8224)  classification: 0.3828 (0.3539)  bbox_regression: 0.4413 (0.4685)  time: 0.4617  data: 0.0102  max mem: 5999\n",
      "Epoch: [1]  [ 300/1180]  eta: 0:06:49  lr: 0.001000  loss: 0.8207 (0.8058)  classification: 0.3050 (0.3378)  bbox_regression: 0.5014 (0.4679)  time: 0.4618  data: 0.0100  max mem: 5999\n",
      "Epoch: [1]  [ 400/1180]  eta: 0:06:02  lr: 0.001000  loss: 0.7754 (0.8021)  classification: 0.3105 (0.3355)  bbox_regression: 0.3943 (0.4666)  time: 0.4608  data: 0.0095  max mem: 5999\n",
      "Epoch: [1]  [ 500/1180]  eta: 0:05:15  lr: 0.001000  loss: 0.5833 (0.7884)  classification: 0.2241 (0.3267)  bbox_regression: 0.3824 (0.4616)  time: 0.4605  data: 0.0090  max mem: 5999\n",
      "Epoch: [1]  [ 600/1180]  eta: 0:04:28  lr: 0.001000  loss: 0.6663 (0.7712)  classification: 0.2679 (0.3181)  bbox_regression: 0.4054 (0.4531)  time: 0.4619  data: 0.0098  max mem: 5999\n",
      "Epoch: [1]  [ 700/1180]  eta: 0:03:42  lr: 0.001000  loss: 0.7974 (0.7917)  classification: 0.3262 (0.3372)  bbox_regression: 0.4555 (0.4544)  time: 0.4672  data: 0.0153  max mem: 5999\n",
      "Epoch: [1]  [ 800/1180]  eta: 0:02:56  lr: 0.001000  loss: 0.6737 (0.7966)  classification: 0.3097 (0.3437)  bbox_regression: 0.3663 (0.4529)  time: 0.4617  data: 0.0095  max mem: 5999\n",
      "Epoch: [1]  [ 900/1180]  eta: 0:02:09  lr: 0.001000  loss: 0.9143 (0.8037)  classification: 0.4781 (0.3507)  bbox_regression: 0.4600 (0.4530)  time: 0.4629  data: 0.0112  max mem: 5999\n",
      "Epoch: [1]  [1000/1180]  eta: 0:01:23  lr: 0.001000  loss: 0.6401 (0.7981)  classification: 0.2283 (0.3484)  bbox_regression: 0.3519 (0.4497)  time: 0.4614  data: 0.0102  max mem: 5999\n",
      "Epoch: [1]  [1100/1180]  eta: 0:00:37  lr: 0.001000  loss: 0.6077 (0.8011)  classification: 0.2702 (0.3542)  bbox_regression: 0.3833 (0.4469)  time: 0.4612  data: 0.0097  max mem: 5999\n",
      "Epoch: [1]  [1179/1180]  eta: 0:00:00  lr: 0.001000  loss: 0.6198 (0.7933)  classification: 0.2306 (0.3492)  bbox_regression: 0.4163 (0.4441)  time: 0.4591  data: 0.0088  max mem: 5999\n",
      "Epoch: [1] Total time: 0:09:06 (0.4631 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "0.6996337298287233\n",
      "0.49221681584355215\n",
      "Epoch Done\n",
      "Epoch: [2]  [   0/1180]  eta: 0:33:17  lr: 0.001000  loss: 0.5274 (0.5274)  classification: 0.1601 (0.1601)  bbox_regression: 0.3673 (0.3673)  time: 1.6932  data: 1.2274  max mem: 5999\n",
      "Epoch: [2]  [ 100/1180]  eta: 0:08:31  lr: 0.001000  loss: 0.5896 (0.6257)  classification: 0.1936 (0.2468)  bbox_regression: 0.3577 (0.3789)  time: 0.4602  data: 0.0097  max mem: 5999\n",
      "Epoch: [2]  [ 200/1180]  eta: 0:07:38  lr: 0.001000  loss: 0.5747 (0.6477)  classification: 0.2128 (0.2559)  bbox_regression: 0.3591 (0.3918)  time: 0.4615  data: 0.0104  max mem: 5999\n",
      "Epoch: [2]  [ 300/1180]  eta: 0:06:50  lr: 0.001000  loss: 0.5625 (0.6573)  classification: 0.2294 (0.2595)  bbox_regression: 0.4262 (0.3978)  time: 0.4619  data: 0.0104  max mem: 5999\n",
      "Epoch: [2]  [ 400/1180]  eta: 0:06:02  lr: 0.001000  loss: 0.7083 (0.6606)  classification: 0.2733 (0.2648)  bbox_regression: 0.3807 (0.3958)  time: 0.4613  data: 0.0105  max mem: 5999\n",
      "Epoch: [2]  [ 500/1180]  eta: 0:05:15  lr: 0.001000  loss: 0.6261 (0.6517)  classification: 0.2310 (0.2618)  bbox_regression: 0.3457 (0.3898)  time: 0.4617  data: 0.0103  max mem: 5999\n",
      "Epoch: [2]  [ 600/1180]  eta: 0:04:29  lr: 0.001000  loss: 0.5758 (0.6507)  classification: 0.1808 (0.2638)  bbox_regression: 0.3526 (0.3869)  time: 0.4632  data: 0.0108  max mem: 5999\n",
      "Epoch: [2]  [ 700/1180]  eta: 0:03:42  lr: 0.001000  loss: 0.7061 (0.6606)  classification: 0.2573 (0.2679)  bbox_regression: 0.4484 (0.3926)  time: 0.4631  data: 0.0114  max mem: 5999\n",
      "Epoch: [2]  [ 800/1180]  eta: 0:02:56  lr: 0.001000  loss: 0.6294 (0.6598)  classification: 0.1978 (0.2653)  bbox_regression: 0.4009 (0.3945)  time: 0.4673  data: 0.0143  max mem: 5999\n",
      "Epoch: [2]  [ 900/1180]  eta: 0:02:09  lr: 0.001000  loss: 0.6010 (0.6553)  classification: 0.2218 (0.2637)  bbox_regression: 0.3315 (0.3916)  time: 0.4651  data: 0.0124  max mem: 5999\n",
      "Epoch: [2]  [1000/1180]  eta: 0:01:23  lr: 0.001000  loss: 0.4297 (0.6481)  classification: 0.1459 (0.2604)  bbox_regression: 0.2648 (0.3877)  time: 0.4619  data: 0.0098  max mem: 5999\n",
      "Epoch: [2]  [1100/1180]  eta: 0:00:37  lr: 0.001000  loss: 0.6372 (0.6465)  classification: 0.2981 (0.2604)  bbox_regression: 0.3207 (0.3861)  time: 0.4620  data: 0.0094  max mem: 5999\n",
      "Epoch: [2]  [1179/1180]  eta: 0:00:00  lr: 0.001000  loss: 0.5335 (0.6469)  classification: 0.1905 (0.2597)  bbox_regression: 0.3095 (0.3872)  time: 0.4597  data: 0.0089  max mem: 5999\n",
      "Epoch: [2] Total time: 0:09:06 (0.4632 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "0.7743288350062864\n",
      "0.6084657642418978\n",
      "Epoch Done\n",
      "Epoch: [3]  [   0/1180]  eta: 0:29:15  lr: 0.000100  loss: 0.4488 (0.4488)  classification: 0.0938 (0.0938)  bbox_regression: 0.3550 (0.3550)  time: 1.4875  data: 1.0257  max mem: 5999\n",
      "Epoch: [3]  [ 100/1180]  eta: 0:08:30  lr: 0.000100  loss: 0.5017 (0.5477)  classification: 0.1750 (0.2146)  bbox_regression: 0.2858 (0.3331)  time: 0.4635  data: 0.0115  max mem: 5999\n",
      "Epoch: [3]  [ 200/1180]  eta: 0:07:38  lr: 0.000100  loss: 0.4435 (0.5350)  classification: 0.1368 (0.2090)  bbox_regression: 0.2959 (0.3260)  time: 0.4618  data: 0.0098  max mem: 5999\n",
      "Epoch: [3]  [ 300/1180]  eta: 0:06:49  lr: 0.000100  loss: 0.3559 (0.5211)  classification: 0.1402 (0.2032)  bbox_regression: 0.2349 (0.3179)  time: 0.4620  data: 0.0102  max mem: 5999\n",
      "Epoch: [3]  [ 400/1180]  eta: 0:06:02  lr: 0.000100  loss: 0.4825 (0.5170)  classification: 0.1727 (0.2024)  bbox_regression: 0.2984 (0.3146)  time: 0.4663  data: 0.0108  max mem: 5999\n",
      "Epoch: [3]  [ 500/1180]  eta: 0:05:15  lr: 0.000100  loss: 0.4225 (0.5160)  classification: 0.1763 (0.2015)  bbox_regression: 0.2666 (0.3145)  time: 0.4610  data: 0.0098  max mem: 5999\n",
      "Epoch: [3]  [ 600/1180]  eta: 0:04:28  lr: 0.000100  loss: 0.4333 (0.5179)  classification: 0.1524 (0.2055)  bbox_regression: 0.2777 (0.3124)  time: 0.4610  data: 0.0100  max mem: 5999\n",
      "Epoch: [3]  [ 700/1180]  eta: 0:03:42  lr: 0.000100  loss: 0.4513 (0.5154)  classification: 0.1260 (0.2044)  bbox_regression: 0.2450 (0.3110)  time: 0.4614  data: 0.0103  max mem: 5999\n",
      "Epoch: [3]  [ 800/1180]  eta: 0:02:56  lr: 0.000100  loss: 0.4052 (0.5132)  classification: 0.1374 (0.2033)  bbox_regression: 0.2853 (0.3100)  time: 0.4619  data: 0.0105  max mem: 5999\n",
      "Epoch: [3]  [ 900/1180]  eta: 0:02:09  lr: 0.000100  loss: 0.3966 (0.5050)  classification: 0.1318 (0.1995)  bbox_regression: 0.2587 (0.3055)  time: 0.4613  data: 0.0097  max mem: 5999\n",
      "Epoch: [3]  [1000/1180]  eta: 0:01:23  lr: 0.000100  loss: 0.4295 (0.5044)  classification: 0.1848 (0.1999)  bbox_regression: 0.2796 (0.3045)  time: 0.4616  data: 0.0094  max mem: 5999\n",
      "Epoch: [3]  [1100/1180]  eta: 0:00:37  lr: 0.000100  loss: 0.4441 (0.5016)  classification: 0.1461 (0.1984)  bbox_regression: 0.2888 (0.3031)  time: 0.4630  data: 0.0108  max mem: 5999\n",
      "Epoch: [3]  [1179/1180]  eta: 0:00:00  lr: 0.000100  loss: 0.4168 (0.5010)  classification: 0.1574 (0.1988)  bbox_regression: 0.2287 (0.3021)  time: 0.4601  data: 0.0090  max mem: 5999\n",
      "Epoch: [3] Total time: 0:09:06 (0.4630 s / it)\n",
      "Train done, evaluating.\n",
      "Inference done, computing mAp : \n",
      "0.8525822487997275\n",
      "0.7376941074593824\n",
      "Epoch Done\n",
      "Epoch: [4]  [   0/1180]  eta: 0:34:02  lr: 0.000100  loss: 0.3360 (0.3360)  classification: 0.1257 (0.1257)  bbox_regression: 0.2103 (0.2103)  time: 1.7313  data: 1.2837  max mem: 5999\n",
      "Epoch: [4]  [ 100/1180]  eta: 0:08:34  lr: 0.000100  loss: 0.4576 (0.4693)  classification: 0.1549 (0.1826)  bbox_regression: 0.2920 (0.2867)  time: 0.4640  data: 0.0103  max mem: 5999\n",
      "Epoch: [4]  [ 200/1180]  eta: 0:07:39  lr: 0.000100  loss: 0.3711 (0.4708)  classification: 0.1458 (0.1904)  bbox_regression: 0.1989 (0.2805)  time: 0.4636  data: 0.0115  max mem: 5999\n",
      "Epoch: [4]  [ 300/1180]  eta: 0:06:51  lr: 0.000100  loss: 0.3518 (0.4659)  classification: 0.1282 (0.1870)  bbox_regression: 0.2242 (0.2789)  time: 0.4609  data: 0.0094  max mem: 5999\n",
      "Epoch: [4]  [ 400/1180]  eta: 0:06:03  lr: 0.000100  loss: 0.3505 (0.4789)  classification: 0.1508 (0.1895)  bbox_regression: 0.2167 (0.2895)  time: 0.4626  data: 0.0098  max mem: 5999\n",
      "Epoch: [4]  [ 500/1180]  eta: 0:05:16  lr: 0.000100  loss: 0.4057 (0.4742)  classification: 0.1365 (0.1861)  bbox_regression: 0.2468 (0.2881)  time: 0.4631  data: 0.0095  max mem: 5999\n",
      "Epoch: [4]  [ 600/1180]  eta: 0:04:29  lr: 0.000100  loss: 0.3686 (0.4745)  classification: 0.1396 (0.1849)  bbox_regression: 0.2372 (0.2896)  time: 0.4640  data: 0.0115  max mem: 5999\n",
      "Epoch: [4]  [ 700/1180]  eta: 0:03:43  lr: 0.000100  loss: 0.4247 (0.4730)  classification: 0.1479 (0.1839)  bbox_regression: 0.2448 (0.2891)  time: 0.4619  data: 0.0101  max mem: 5999\n",
      "Epoch: [4]  [ 800/1180]  eta: 0:02:56  lr: 0.000100  loss: 0.4505 (0.4732)  classification: 0.1565 (0.1850)  bbox_regression: 0.2709 (0.2882)  time: 0.4613  data: 0.0092  max mem: 5999\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n",
    "    print (\"Train done, evaluating.\")\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    inference_res = evaluate(model,data_loader_val)\n",
    "    print('Inference done, computing mAp : ')\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.6, score_threshold = 0.05))\n",
    "    print('Epoch Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e777a8-100b-4614-b7aa-dcfdd012117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After first epoch 'scores' in predictions was empty, which was reason for 0 mAp. If it will stay the same over several epoch - need to debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
