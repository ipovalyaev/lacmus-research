{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47fa14dc",
   "metadata": {},
   "source": [
    "# This code is gonna be obsolete, for dataset transformation to Yolo v5 look at EDA/LADDvsIPSAR_merge_and_prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e99e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_voc_xml(node: ET.Element) -> Dict[str, Any]:\n",
    "    voc_dict: Dict[str, Any] = {}\n",
    "    children = list(node)\n",
    "    if children:\n",
    "        def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
    "        for dc in map(parse_voc_xml, children):\n",
    "            for ind, v in dc.items():\n",
    "                def_dic[ind].append(v)\n",
    "        if node.tag == 'annotation':\n",
    "            def_dic['object'] = [def_dic['object']]\n",
    "        voc_dict = {\n",
    "            node.tag:\n",
    "                {ind: v[0] if len(v) == 1 else v\n",
    "                 for ind, v in def_dic.items()}\n",
    "        }\n",
    "    if node.text:\n",
    "        text = node.text.strip()\n",
    "        if not children:\n",
    "            voc_dict[node.tag] = text\n",
    "    return voc_dict\n",
    "\n",
    "\n",
    "def get_imgSize_and_list_of_yxyx(xml_path):\n",
    "    mytree = parse_voc_xml(ET.parse(xml_path).getroot())\n",
    "    xyxy=[]\n",
    "    for p in mytree['annotation']['object']:\n",
    "        y1=int(p['bndbox']['ymin'])\n",
    "        x1=int(p['bndbox']['xmin'])\n",
    "        y2=int(p['bndbox']['ymax'])\n",
    "        x2=int(p['bndbox']['xmax'])\n",
    "        xyxy.append(((x1,y1),(x2,y2)))\n",
    "    size=(int(mytree['annotation']['size']['width']),int(mytree['annotation']['size']['height']))\n",
    "    return size, xyxy\n",
    "\n",
    "\n",
    "\n",
    "def get_xywh_from_point(size,points):\n",
    "    w,h=size\n",
    "    x=((points[0][0]+points[1][0])/2)/w\n",
    "    y=((points[0][1]+points[1][1])/2)/h\n",
    "    im_w=abs((points[0][0]-points[1][0]))/w\n",
    "    im_h=abs((points[0][1]-points[1][1]))/h\n",
    "    return x,y,im_w,im_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6bcaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dir='./data/TrainingData/labels'\n",
    "anotation_path = '../../ladd-and-weights/dataset/full_train_ds/Annotations/'\n",
    "\n",
    "os.makedirs(labels_dir,exist_ok=True)\n",
    "for xml_name in os.listdir(anotation_path):\n",
    "    id,_ = os.path.splitext(xml_name)\n",
    "    img_size, points_yxyx=get_imgSize_and_list_of_yxyx(os.path.join(anotation_path,xml_name))\n",
    "    xywhs=[get_xywh_from_point(img_size,xyxy_single) for xyxy_single in points_yxyx]\n",
    "    f=open(os.path.join(labels_dir,id+'.txt'),'w')\n",
    "    for box in xywhs:\n",
    "        f.write(\" \".join(['0',*map(str,box)])+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images_path='./data/images'\n",
    "dataset_labels_path='./data/labels'\n",
    "os.makedirs(dataset_images_path,exist_ok=True)\n",
    "os.makedirs(dataset_labels_path, exist_ok=True)\n",
    "\n",
    "# train_file=open('../../ladd-and-weights/dataset/full_train_ds/ImageSets/Main/train_non_empty.txt','r')\n",
    "train_file=open('../../ladd-and-weights/dataset/full_train_ds/ImageSets/Main/train.txt','r')\n",
    "\n",
    "for id in train_file.readlines():\n",
    "    id=id.strip()\n",
    "    images_train_path=os.path.join(dataset_images_path,'train')\n",
    "    labels_train_path=os.path.join(dataset_labels_path,'train')\n",
    "    os.makedirs(images_train_path,exist_ok=True)  \n",
    "    os.makedirs(labels_train_path,exist_ok=True)\n",
    "    shutil.copy(os.path.join('../../ladd-and-weights/dataset/full_train_ds/JPEGImages',id+'.jpg')\n",
    "              ,os.path.join(images_train_path,id+'.jpg'))\n",
    "    shutil.copy(os.path.join('./data/TrainingData/labels/',id+'.txt')\n",
    "              ,os.path.join(labels_train_path,id+'.txt'))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b560a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file=open('../../ladd-and-weights/dataset/full_train_ds/ImageSets/Main/val.txt','r')\n",
    "for id in val_file.readlines():\n",
    "    id=id.strip()\n",
    "    images_train_path=os.path.join(dataset_images_path,'valid')\n",
    "    labels_train_path=os.path.join(dataset_labels_path,'valid')\n",
    "    os.makedirs(images_train_path,exist_ok=True)  \n",
    "    os.makedirs(labels_train_path,exist_ok=True)\n",
    "    shutil.copy(os.path.join('../../../git/ladd-and-weights/dataset/full_train_ds/JPEGImages',id+'.jpg')\n",
    "              ,os.path.join(images_train_path,id+'.jpg'))\n",
    "    shutil.move(os.path.join('./data/TrainingData/labels/',id+'.txt')\n",
    "              ,os.path.join(labels_train_path,id+'.txt'))    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7f6ed-fca1-4d61-84d3-75a51df63590",
   "metadata": {},
   "source": [
    "# script for network training\n",
    "Adjust batch size, workers according to your GPU and CPU resources  \n",
    "`python ./yolov5/train.py --hyp ./yolo5_settings/hyp_lacmus.yaml --batch-size 8 --worker 16 \\ `  \n",
    "`--data ./yolo5_settings/dataset_lacmus_local.yaml --weights yolov5s.pt --imgsz 1984 --epochs 50`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64e5a2-9312-4ee5-adc1-b2ebe5e8870b",
   "metadata": {},
   "source": [
    "# export to tensorflow\n",
    "`python ./yolov5/export.py --data ./yolo5_settings/dataset_lacmus_local.yaml --weights ./exp13/weights/best.pt \\ `  \n",
    "`--img-size 1984 --dynamic --include pb --conf-thres 0.05 --iou-thres 0.2 --batch 1 --nms`\n",
    "\n",
    "# Then you can create onnx based on exported model if you (there is no option to export onnx with nms\n",
    "`python -m tf2onnx.convert --saved-model ./exp13/weights/best_saved_model --opset 13 --output tf2onnx_best.onnx`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
