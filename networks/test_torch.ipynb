{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faf73ca5-02c9-41a5-9a50-2c67a4af0c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.detection.retinanet import RetinaNet\n",
    "import  torchvision.transforms.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7290ce25-a74d-4ad2-8a1f-9fca5ae1eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    This function helps when we have different number of object instances\n",
    "    in the batches in the dataset.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff385bc-f4f5-41c0-b172-a1860392d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "    \n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = image.flip(-1)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n",
    "    \n",
    "class Resize(object):\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def __call__(self, image, target):\n",
    "        old_height, old_width = image.shape[-2:]\n",
    "        image = F.resize(image, self.target_size, interpolation=F.InterpolationMode.BILINEAR)\n",
    "        new_height, new_width = image.shape[-2:]\n",
    "        bbox = target[\"boxes\"]\n",
    "        bbox[:, [0, 2]] = bbox[:, [0, 2]] * new_width / old_width\n",
    "        bbox[:, [1, 3]] = bbox[:, [1, 3]] * new_height / old_height\n",
    "        target[\"boxes\"] = bbox\n",
    "        \n",
    "        \n",
    "        return image, target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ecf557-2341-464f-b370-acc009ffafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(ToTensor())\n",
    "    transforms.append(Resize((800,1333)))\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(RandomHorizontalFlip(0.5))\n",
    "    return Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b63ba9b-2b9f-4073-96b6-2cfdad94eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDDDataSET(torch.utils.data.Dataset):\n",
    "    def __init__(self, typeOfDS, transforms=None):\n",
    "        self.typeOfDS = typeOfDS\n",
    "        self.labels = pd.read_csv('/app/host/lacmus/dataset/sdd-lacmus-version/%s_annotations_pedestrian.csv'%typeOfDS, header = None, \n",
    "            names = ['image','x0','y0','x1','y1','class'])\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.image.nunique()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_name = self.labels.image.unique()[idx]\n",
    "        img_labels = self.labels [self.labels.image == img_name]\n",
    "        img = Image.open('/app/host/lacmus/dataset/sdd-lacmus-version/'+img_name).convert(\"RGB\") #convert from tutorial, do we need it?\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = img_labels.shape[0]\n",
    "        boxes = []\n",
    "        for l in img_labels.iterrows():\n",
    "            boxes.append([l[1]['x0'], l[1]['y0'], l[1]['x1'], l[1]['y1']])\n",
    "\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)         # there is only one class\n",
    "        target[\"labels\"] = labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72369d23-c361-414b-8557-1deec22bdbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_idx = 10\n",
    "\n",
    "# dataset = SDDDataSET('test',get_transform(train=True)) \n",
    "# (image,target) = dataset[im_idx] \n",
    "# im = F.to_pil_image(image)\n",
    "# draw = ImageDraw.Draw(im)\n",
    "\n",
    "# for bb in target['boxes']:\n",
    "#     draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "#                (bb[2], bb[1]), (bb[0], bb[1])], width=4, fill=(255, 0, 0))\n",
    "\n",
    "# im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc868c76-562f-40aa-9d75-441cc9912d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = SDDDataSET('train', get_transform(train=True))\n",
    "dataset_val = SDDDataSET('val', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=1, shuffle=True, num_workers=4\n",
    "     ,collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, batch_size=1, shuffle=False, num_workers=4\n",
    "     ,collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4891861-a61f-4c64-883d-021ccd279dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an instance segmentation model pre-trained on COCO    \n",
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=2, pretrained_backbone=True)\n",
    "\n",
    "# the computation device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05c78bc8-d88a-48de-8088-5f2c41474ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import time\n",
    "import datetime\n",
    "import torch.distributed as dist\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "        \n",
    "        \n",
    "\n",
    "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
    "\n",
    "    def f(x):\n",
    "        if x >= warmup_iters:\n",
    "            return 1\n",
    "        alpha = float(x) / warmup_iters\n",
    "        return warmup_factor * (1 - alpha) + alpha\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "def reduce_dict(input_dict, average=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dict (dict): all the values will be reduced\n",
    "        average (bool): whether to do average or sum\n",
    "    Reduce the values in the dictionary from all processes so that all processes\n",
    "    have the averaged results. Returns a dict with the same fields as\n",
    "    input_dict, after reduction.\n",
    "    \"\"\"\n",
    "    world_size = get_world_size()\n",
    "    if world_size < 2:\n",
    "        return input_dict\n",
    "    with torch.no_grad():\n",
    "        names = []\n",
    "        values = []\n",
    "        # sort the keys so that they are consistent across processes\n",
    "        for k in sorted(input_dict.keys()):\n",
    "            names.append(k)\n",
    "            values.append(input_dict[k])\n",
    "        values = torch.stack(values, dim=0)\n",
    "        dist.all_reduce(values)\n",
    "        if average:\n",
    "            values /= world_size\n",
    "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
    "    return reduced_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e09685db-2177-47d4-a84e-fd2e0e2d8532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8bd293-8950-4ecf-8412-2ae54162ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device='cuda'):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    inference_res = []\n",
    "    model.eval()\n",
    "    \n",
    "    for images, targets in data_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        res = targets, outputs\n",
    "        inference_res.append(res)\n",
    "\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return inference_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "747ed96a-b8a2-4337-b7c3-bdae334d154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], recall, [1.]))\n",
    "    mpre = np.concatenate(([0.], precision, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97c204f2-fa70-47db-a8e7-3321a023351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyximport\n",
    "pyximport.install()\n",
    "from compute_overlap import compute_overlap\n",
    "\n",
    "\n",
    "def evaluate_res(\n",
    "    inference_res,\n",
    "    iou_threshold=0.5,\n",
    "    score_threshold=0.05,\n",
    "    max_detections=100\n",
    "):\n",
    "    \"\"\" Evaluate a given dataset using a given model.\n",
    "    # Arguments\n",
    "        iou_threshold   : The threshold used to consider when a detection is positive or negative.\n",
    "        score_threshold : The score confidence threshold to use for detections.\n",
    "        max_detections  : The maximum number of detections to use per image.\n",
    "    \"\"\"\n",
    "    # gather all detections and annotations\n",
    "#     all_detections, all_inferences = \\\n",
    "#         _get_detections(generator, model, score_threshold=score_threshold, max_detections=max_detections, save_path=save_path)\n",
    "#     all_annotations    = _get_annotations(generator)\n",
    "#     average_precisions = {}\n",
    "\n",
    "    false_positives = np.zeros((0,))\n",
    "    true_positives  = np.zeros((0,))\n",
    "    scores          = np.zeros((0,))\n",
    "    num_annotations = 0.0\n",
    "\n",
    "    for i in range(len(inference_res)):\n",
    "        detections           = inference_res[i][1][0]\n",
    "        annotations          = inference_res[i][0][0]\n",
    "        num_annotations     += inference_res[i][0][0]['labels'].shape[0]\n",
    "        detected_annotations = []\n",
    "\n",
    "        for d in range(detections['labels'].shape[0]):\n",
    "            if detections['scores'][d].numpy() > score_threshold:\n",
    "                scores = np.append(scores, detections['scores'][d].numpy())\n",
    "\n",
    "                if inference_res[i][0][0]['labels'].shape[0] == 0: # no objects was there\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "                    continue\n",
    "\n",
    "                overlaps            = compute_overlap (np.expand_dims(detections['boxes'][d].numpy().astype(np.double),axis=0),annotations['boxes'].numpy().astype(np.double))\n",
    "                assigned_annotation = np.argmax(overlaps, axis=1)\n",
    "                max_overlap         = overlaps[0, assigned_annotation][0]\n",
    "\n",
    "                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n",
    "                    false_positives = np.append(false_positives, 0)\n",
    "                    true_positives  = np.append(true_positives, 1)\n",
    "                    detected_annotations.append(assigned_annotation)\n",
    "                else:\n",
    "                    false_positives = np.append(false_positives, 1)\n",
    "                    true_positives  = np.append(true_positives, 0)\n",
    "\n",
    "\n",
    "#     # sort by score\n",
    "    indices         = np.argsort(-scores)\n",
    "    false_positives = false_positives[indices]\n",
    "    true_positives  = true_positives[indices]\n",
    "\n",
    "#     # compute false positives and true positives\n",
    "    false_positives = np.cumsum(false_positives)\n",
    "    true_positives  = np.cumsum(true_positives)\n",
    "\n",
    "#     # compute recall and precision\n",
    "    recall    = true_positives / num_annotations\n",
    "    precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n",
    "\n",
    "    # compute average precision\n",
    "    average_precision  = _compute_ap(recall, precision)\n",
    "\n",
    "\n",
    "    return average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deefc4c-87ff-46e6-9bd3-ae7f29fbaa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/2556]  eta: 0:24:56  lr: 0.000002  loss: 1.8189 (1.8189)  classification: 1.1299 (1.1299)  bbox_regression: 0.6890 (0.6890)  time: 0.5856  data: 0.3083  max mem: 1096\n",
      "Epoch: [0]  [ 500/2556]  eta: 0:08:51  lr: 0.000501  loss: 1.0900 (1.6261)  classification: 0.5608 (1.0044)  bbox_regression: 0.5301 (0.6217)  time: 0.2629  data: 0.0044  max mem: 1523\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=500)\n",
    "    print (\"Train done, evaluating.\")\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    inference_res = evaluate(model,data_loader_val)\n",
    "    print('Inference done, computing mAp : ')\n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.5, score_threshold = 0.05))    \n",
    "    print(evaluate_res(inference_res, iou_threshold = 0.6, score_threshold = 0.05))\n",
    "    print('Epoch Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4be94-1b5d-4242-8cf9-c26d6e0b6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_device = torch.device(\"cpu\")\n",
    "model.eval()\n",
    "for images, targets in data_loader:\n",
    "    g_images = list(img.to(device) for img in images)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    outputs = model(g_images)\n",
    "\n",
    "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "    res = targets, outputs\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9c93f-c1fb-4537-ba20-24c2346782a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = to_pil_image(images[0])\n",
    "targets\n",
    "# im = to_pil_image(dataset[10][0])\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "for bb in outputs[0]['boxes'][:10]:\n",
    "    draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "               (bb[2], bb[1]), (bb[0], bb[1])], width=4, fill=(255, 0, 0))\n",
    "\n",
    "for bb in targets[0]['boxes'][:10]:\n",
    "    draw.line([(bb[0], bb[1]), (bb[0], bb[3]), (bb[2], bb[3]),\n",
    "               (bb[2], bb[1]), (bb[0], bb[1])], width=4, fill=(0,255, 0))\n",
    "im.show()\n",
    "\n",
    "# This suggests test set is far from perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18892ede-0ebb-416e-b482-e16646cf1efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
